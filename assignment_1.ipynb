{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT EDIT THIS FILE WITHIN THE /TSDS FOLDER - YOU RISK OVERWRITING YOUR WORK THE NEXT TIME YOU PULL FROM THE GITHUB REPOSITORY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group members:\n",
    "## Sina Smid, Edith Zink, Zeyu Zhao, Helge Zille\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "\n",
    "### Practical info\n",
    "* Handin in absalon. The deadline is the 5th of march (see the [course plan](https://github.com/abjer/tsds/wiki/Course-plan))\n",
    "* You must work in groups of 2-4. **Remember to identify the group members in the filename or in the top of the file contents**.\n",
    "* If anything is unclear dont hesitate to email me at kuol@econ.ku.dk with questions.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 1 (ML recap)\n",
    "The following questions are drawn from exercise set 1. We have included code that allows you to solve the questions independently of the previous questions in exercise set 1. Note that you might have solved the questions in a different way than we anticipated. In this case the supplied code might need some modification to work with your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=10000000000, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'clf__C': array([1.e-04, 1.e-02, 1.e+00, 1.e+02, 1.e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: there are three .zip files with letter = a,b,c. \n",
    "# To ensure the files download in reasonable time we \n",
    "# only work with the first of the three. If you have time\n",
    "# you can modify this cell to download all three. \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler        # scales variables to be mean=0,sd=1\n",
    "from sklearn.linear_model import LogisticRegression     # regression model\n",
    "from sklearn.pipeline import Pipeline                   # For building our model pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "filenames = []\n",
    "base_url = 'https://resources.lendingclub.com/'\n",
    "\n",
    "letter = 'a'\n",
    "filename = f'LoanStats3{letter}.csv.zip'\n",
    "url = base_url+filename\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)\n",
    "filenames.append(filename)\n",
    "\n",
    "# Read in csv files, store them\n",
    "dfs = [pd.read_csv(f,header=0,skiprows=1,low_memory=False) for f in filenames]\n",
    "\n",
    "# concatenate the dataframes (as standard there is only 1)\n",
    "df = pd.concat(dfs)\\\n",
    "        .dropna(subset=['loan_amnt'])\\\n",
    "        .dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "# Identify loans of interest\n",
    "df = df.loc[df.loan_status.isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "\n",
    "# Clean up variables \n",
    "df['charged_off'] = (df.loan_status=='Charged Off').astype(int)\n",
    "df['int_rate_f'] = df.int_rate.str[:-1].astype(float)\n",
    "df['emp_length_f'] = df.emp_length\\\n",
    "                        .str.split(' ')\\\n",
    "                        .str[0].str[:2]\\\n",
    "                        .str.replace('<','0')\\\n",
    "                        .astype(float)\n",
    "\n",
    "# label and features\n",
    "y_var = 'charged_off'\n",
    "X_vars = ['term', 'int_rate_f', 'grade', 'home_ownership', 'emp_length_f',\n",
    "          'annual_inc', 'verification_status', 'dti']\n",
    "\n",
    "# Create dummies\n",
    "data = pd.get_dummies(df[X_vars+[y_var]], drop_first=True)\\\n",
    "        .dropna()\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .astype(np.float64)\\\n",
    "        .loc[:2000]\\\n",
    "        .copy()\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=.3, random_state=3)\n",
    "\n",
    "# These are the row indices of the stratified split\n",
    "data_splits = list(sss.split(data[y_var], data[y_var]))\n",
    "\n",
    "# Separate data in y,X\n",
    "y = data[y_var]\n",
    "X_vars_b = data.columns!=y_var\n",
    "X = data.loc[:,X_vars_b]\n",
    "\n",
    "train_idx, test_idx = data_splits[0]\n",
    "\n",
    "y_train = y.loc[train_idx]\n",
    "X_train = X.loc[train_idx]\n",
    "\n",
    "y_test = y.loc[test_idx]\n",
    "X_test = X.loc[test_idx]\n",
    "\n",
    "\n",
    "# Fit vanilla linear model\n",
    "lr = Pipeline([('scale', StandardScaler()),\n",
    "               ('clf', LogisticRegression(class_weight='balanced',C=10**10, solver = 'liblinear'))])\n",
    "\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Fit linear model with CV\n",
    "lr_cv = GridSearchCV(estimator=lr,\n",
    "                     param_grid={'clf__C':np.logspace(-4,4,5)},\n",
    "                     n_jobs=-1,\n",
    "                     cv=3)\n",
    "lr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.8:** Apply nested resampling to compute a distribution of test scores with and without optimization. You should use `data_splits` which we defined initially and input all the data.\n",
    ">\n",
    ">> *Hint:* You can implement this using your code from Ex. 1.1.6 and combine it with `cross_val_score`. Note that `cv` input should use `data_splits`. See Raschka pp. 188-189 for inspiration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.33727811 0.40233236 0.32       0.36612022 0.35164835 0.3452381\n",
      " 0.3559322  0.37317784 0.38692098 0.37082067]\n",
      "CV accuracy: 0.361 +/- 0.023\n",
      "CV accuracy scores optimized: [0.3373494  0.40121581 0.3003663  0.38616715 0.39889197 0.33438486\n",
      " 0.34857143 0.35555556 0.38068182 0.3322884 ]\n",
      "CV accuracy optimized: 0.358 +/- 0.031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cvs=cross_val_score(lr,X,y,cv=data_splits,scoring='f1')\n",
    "\n",
    "cvs_opt=cross_val_score(lr_cv,X,y,cv=data_splits,scoring='f1')\n",
    "\n",
    "print('CV accuracy scores: %s'% cvs)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(cvs), np.std(cvs)))\n",
    "\n",
    "print('CV accuracy scores optimized: %s'% cvs_opt)\n",
    "print('CV accuracy optimized: %.3f +/- %.3f' % (np.mean(cvs_opt), np.std(cvs_opt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33727811 0.40233236 0.32       0.36612022 0.35164835 0.3452381\n",
      " 0.3559322  0.37317784 0.38692098 0.37082067]\n",
      "[0.3373494  0.40121581 0.3003663  0.38616715 0.39889197 0.33438486\n",
      " 0.34857143 0.35555556 0.38068182 0.3322884 ]\n"
     ]
    }
   ],
   "source": [
    "#### EDITHS SOLUTION:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nested_cv_score_nonopt = cross_val_score(estimator= lr, \n",
    "                                         X=X, y=y, \n",
    "                                         cv=data_splits,\n",
    "                                         scoring='f1',\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "nested_cv_score_opt = cross_val_score(estimator= lr_cv, \n",
    "                                      X=X, y=y,\n",
    "                                      cv=data_splits, \n",
    "                                      scoring='f1',\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "print(nested_cv_score_nonopt)\n",
    "print(nested_cv_score_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.11** Estimate a classification tree on the training data (with default hyperparameters). Evalate both on training and test data by computing the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You can check out code for Ex. 1.1.10 for inspiration. You may also want to look up `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (training set): 1.0\n",
      "AUC (testing set): 0.5008135003216164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "\n",
    "from sklearn import metrics #Import scikit-learn metrics module for area under the curve calculation\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for train and test dataset\n",
    "y_score_train = clf.predict_proba(X_train)\n",
    "y_score_test = clf.predict_proba(X_test)\n",
    "\n",
    "print(\"AUC (training set):\",metrics.roc_auc_score(y_train, y_score_train[:,1]))\n",
    "print(\"AUC (testing set):\",metrics.roc_auc_score(y_test, y_score_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.13** Is Random Forest classification different from the procedure of aggregating tree predictions above? If so, explain how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex.1.1.13 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are different because Random Forests also do feature bagging (= bootstrap which features to use).\n",
    "The idea behind Random Forests is to average multiple DCTs to build a more robust model that has better generalization performance. \n",
    "A Random Forest model runs the following steps:\n",
    "1. Draw a random bootstrap sample.\n",
    "2. Grow decision tree from the bootstrap sample\n",
    "  1. randomly select a fixed number of features without replacement\n",
    "  2. split the node using the feature that provides best split\n",
    "3. Repeat 1. and 2.\n",
    "4. Aggregate the results of each tree via majority vote.\n",
    "\n",
    "Obviously, Random Forest classification is different from the aggregating procedure in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 2 (ANN 1)\n",
    "The following questions are drawn from exercise set 2. We have included code that allows you to solve the questions independently of the previous questions in exercise set 1. Note that you might have solved the questions in a different way than we anticipated. In this case the supplied code might need some modification to work with your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def step(z, threshold=0.5):\n",
    "    if z > threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Feed forward neural network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Q: Print these out, explain their contents. You can instantiate a network by\n",
    "        # doing `net = Network([2, 3, 1])`, and then printing `net.biases`.\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        \n",
    "        # Q: What is `a`? How many iterations will this loop run? For a `sizes=[2, 3, 1]`\n",
    "        # network, what is the shape of `a` at each iteration?\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, silent=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        \n",
    "        n = len(training_data)\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            # Q: What happens here? Why do we shuffle the training data? Explain the\n",
    "            # contents of `mini_batches`.\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            \n",
    "            # Q: And what does this step do?\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if not silent:\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        \n",
    "        # Q: These two vectors correspond to -∇C(W) (and -∇C(b))\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Q: But what happens here? Explain, in particular, how we update `nabla_b` and `nabla_w`\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # Q: Now we have our gradient vectors, `nabla_b` and `nabla_w`. Explain how we use them\n",
    "        # to update the weights and biases\n",
    "        self.weights = [\n",
    "            w - eta / len(mini_batch) * nw\n",
    "            for w, nw in zip(self.weights, nabla_w)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - eta / len(mini_batch) * nb\n",
    "            for b, nb in zip(self.biases, nabla_b)\n",
    "        ]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        if self.sizes[-1] == 1:\n",
    "            test_results = [\n",
    "                (step(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        else:\n",
    "            test_results = [\n",
    "                (np.argmax(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        return sum(int(y_pred == y) for (y_pred, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return output_activations - y\n",
    "    \n",
    "    \n",
    "# Load in the MNIST data used for ex. 2.2.x    \n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "plt.imshow(training_data[0][0].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 2.1.2** Using [the dataset](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.16631&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) with the hard-to-seperate circles, create the minimal neural network that seperates the clusters. Again, report your answer with a link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.16631&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 2.2.4**: Now, fit a model with a suiting architecture (i.e. `sizes`) to `training_data`, and report your accuracy on the `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9105 / 10000\n",
      "Epoch 1: 9295 / 10000\n",
      "Epoch 2: 9346 / 10000\n",
      "Epoch 3: 9388 / 10000\n",
      "Epoch 4: 9413 / 10000\n",
      "Epoch 5: 9447 / 10000\n",
      "Epoch 6: 9456 / 10000\n",
      "Epoch 7: 9473 / 10000\n",
      "Epoch 8: 9481 / 10000\n",
      "Epoch 9: 9480 / 10000\n",
      "Epoch 10: 9492 / 10000\n",
      "Epoch 11: 9488 / 10000\n",
      "Epoch 12: 9511 / 10000\n",
      "Epoch 13: 9506 / 10000\n",
      "Epoch 14: 9512 / 10000\n",
      "Epoch 15: 9521 / 10000\n",
      "Epoch 16: 9498 / 10000\n",
      "Epoch 17: 9513 / 10000\n",
      "Epoch 18: 9530 / 10000\n",
      "Epoch 19: 9533 / 10000\n",
      "Epoch 20: 9525 / 10000\n",
      "Epoch 21: 9537 / 10000\n",
      "Epoch 22: 9539 / 10000\n",
      "Epoch 23: 9551 / 10000\n",
      "Epoch 24: 9535 / 10000\n",
      "Epoch 25: 9532 / 10000\n",
      "Epoch 26: 9510 / 10000\n",
      "Epoch 27: 9537 / 10000\n",
      "Epoch 28: 9539 / 10000\n",
      "Epoch 29: 9550 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9550"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to ex. 2.2.4]\n",
    "\n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=validation_data)\n",
    "\n",
    "net.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 2.2.5**: Assuming you could get a \"pretty\" high accuracy in Ex. 2.2.4, Visualize 10 examples that get misclassified (remember to write what the correct label is). comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict this to be 8\n"
     ]
    }
   ],
   "source": [
    "# [Answer to ex. 2.2.5]\n",
    "\n",
    "y_train = [training_data[i][1] for i in range(len(training_data))]\n",
    "X_train = [training_data[i][0] for i in range(len(training_data))]\n",
    "\n",
    "y_validation = [training_data[i][1] for i in range(len(validation_data))]\n",
    "X_validation = [training_data[i][0] for i in range(len(validation_data))]\n",
    "\n",
    "y_pred = [net.feedforward(x) for x in X_validation]\n",
    "\n",
    "ind_pred =[np.argmax(y_pred[i]) for i in range(len(y_pred))]\n",
    "ind_true =[np.argmax(y_validation[i]) for i in range(len(y_validation))]\n",
    "\n",
    "li=[]\n",
    "\n",
    "for i in range(len(y_validation)):\n",
    "    if ind_pred[i]!=ind_true[i]:\n",
    "        li=li+[i]\n",
    "        \n",
    "for i in range(10):\n",
    "    plt.imshow(training_data[li[i]][0].reshape(28,28))\n",
    "    plt.show()\n",
    "    print('We predict this to be ' + str(ind_pred[li[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 3 (ANN 2)\n",
    "The following questions are drawn from exercise set 3. \n",
    "\n",
    "![img](https://raw.githubusercontent.com/abjer/tsds/master/material_exercises/week_3/2_3_1_net.png)\n",
    "\n",
    "**Ex. 3.1.2**: Knowing about backpropagation, we actually have everything we need here to compute the gradients of the weights by hand. So go ahead and do that. Report your answer either as a diagram that includes the gradients (you can draw on my figure somehow and insert the resulting image), or just by writing what the gradient of each weight is.\n",
    ">\n",
    "> *Hint: When computing gradients with backprop, it can be a bit easier to think of the network as a computational graph. My computational graph looks like [this](https://github.com/abjer/tsds/blob/master/material_exercises/week_3/2_3_1_net_compgraph.png?raw=true).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 3.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following general formula to calculate the gradient for weight $w_{jk}^{l}$ to the hidden layer, hence, the weight for the connection from the $k_{th}$ neuron in the input layer to the $j_{th}$ neuron in the hidden layer $l$. In the formula $L$ indicates the output layer:\n",
    "$$ \\frac{\\partial C}{\\partial w_{jk}^{l}} = \\frac{\\partial C}{\\partial a^{L}} \\frac{\\partial a^{L}}{\\partial a^{l}_{j}} \\frac{\\partial a^{l}_{j}}{\\partial w^{l}_{jk}} $$\n",
    "\n",
    "where\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C}{\\partial a^{L}} &= 2*(a^{L} - 1) \\\\\n",
    "\\frac{\\partial a^{L}}{\\partial a^{l}_{j}} &= w_{j}^{L} \\sigma^{'}(z^{L}) \\\\\n",
    "\\frac{\\partial a^{l}_{j}}{\\partial w^{l}_{jk}} &= x_{k} \\sigma^{'}(z^{l}_{j}) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The gradient for the weights from hidden to output layer are computed as follows:\n",
    "$$ \\frac{\\partial C}{\\partial w_{j}^{L}} = \\frac{\\partial C}{\\partial a^{L}} \\frac{\\partial a^{L}}{\\partial w^{L}_{j}}$$\n",
    "where $$\\frac{\\partial a^{L}}{\\partial w^{L}_{j}} = a^{l}_{j} \\sigma^{'}(z^{L}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the weights to the hidden layer, first input -0.0371 (first neuron), 0.0 (second neuron), -0.0 (third neuron)\n",
      "Gradients for the weights to the hidden layer, second input -0.0185 (first neuron), 0.0 (second neuron), -0.0 (third neuron)\n",
      "Gradients for the weights to the output layer -0.1154 (first neuron), -0.11617 (second neuron), -0.0 (third neuron)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "z1_1 = 4*0.5 + 2*1 + 1\n",
    "z1_2 = 4*2 + 2*10 - 1\n",
    "z1_3= 4*(-5) + 2*0.3 + 2\n",
    "\n",
    "a1_1 = sigmoid(z1_1)\n",
    "a1_2 = sigmoid(z1_2)\n",
    "a1_3 = sigmoid(z1_3)\n",
    "\n",
    "zL = 12*a1_1 - 8*a1_2 + 0.2*a1_3 - 3\n",
    "aL = sigmoid(zL)\n",
    "\n",
    "### The gradients for all the weights are:\n",
    "cost_derivative = 2*(aL - 1)\n",
    "\n",
    "w1_11 = cost_derivative * 12 * sigmoid_derivative(zL) * 4 * sigmoid_derivative(z1_1)\n",
    "w1_21 = cost_derivative * (-8) * sigmoid_derivative(zL) * 4 * sigmoid_derivative(z1_2)\n",
    "w1_31 = cost_derivative * 0.2 * sigmoid_derivative(zL) * 4 * sigmoid_derivative(z1_3)\n",
    "w1_12 = cost_derivative * 12 * sigmoid_derivative(zL) * 2 * sigmoid_derivative(z1_1)\n",
    "w1_22 = cost_derivative * (-8) * sigmoid_derivative(zL) * 2 * sigmoid_derivative(z1_2)\n",
    "w1_32 = cost_derivative * 0.2 * sigmoid_derivative(zL) * 2 * sigmoid_derivative(z1_3)\n",
    "\n",
    "wL_1 = cost_derivative * a1_1 * sigmoid_derivative(zL)\n",
    "wL_2 = cost_derivative * a1_2 * sigmoid_derivative(zL)\n",
    "wL_3 = cost_derivative * a1_3 * sigmoid_derivative(zL)\n",
    "\n",
    "print(\"Gradients for the weights to the hidden layer, first input\", round(w1_11,5), \"(first neuron),\", \n",
    "                      round(w1_21,5), \"(second neuron),\", round(w1_31,5), \"(third neuron)\")\n",
    "print(\"Gradients for the weights to the hidden layer, second input\", round(w1_12,5), \"(first neuron),\", \n",
    "                      round(w1_22,5), \"(second neuron),\", round(w1_32,5), \"(third neuron)\")\n",
    "print(\"Gradients for the weights to the output layer\", round(wL_1,5), \"(first neuron),\", \n",
    "                      round(wL_2,5), \"(second neuron),\", round(wL_3,5), \"(third neuron)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Inked2_3_1_net_compgraph_backward_SJS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5700438403325201\n",
      "-0.11619136654182528\n",
      "-0.02323827330836506\n",
      "0.9295309323346023\n",
      "-1.3942963985019035\n",
      "-0.27502087225105226\n",
      "0.18284131226372166\n",
      "-0.005809568327091265\n",
      "-0.13751043612552613\n",
      "-0.27502087225105226\n",
      "0.3656826245274433\n",
      "1.8284131226372167\n",
      "0.029047841635456324\n",
      "0.0017428704981273795\n"
     ]
    }
   ],
   "source": [
    "# SINA SOLUTION: Backpropagation\n",
    "# Sigmoid function\n",
    "from math import exp, expm1\n",
    "def sig(x):\n",
    "    x1=1/(1+exp(-x))\n",
    "    return(x1)\n",
    "# Derivative Sigmoid function\n",
    "from math import exp, expm1\n",
    "def sig_derivative(x):\n",
    "    x1=sig(x)*(1-sig(x))\n",
    "    return(x1)\n",
    "\n",
    "#Backpropagation - Level 1\n",
    "b1=2*(0.71497807983374-1)\n",
    "print(b1)\n",
    "\n",
    "#Backpropagation - Level 2\n",
    "b2=sig_derivative(0.919)*-0.57\n",
    "print(b2)\n",
    "\n",
    "#Backpropagation - Level 3 - where it splits up\n",
    "b3=b2*0.2\n",
    "print(b3)\n",
    "\n",
    "b4=b2*-8\n",
    "print(b4)\n",
    "\n",
    "b5=b2*12\n",
    "print(b5)\n",
    "\n",
    "#Backpropagation - Level 4\n",
    "b6=sig_derivative(0.993)*b5\n",
    "print(b6)\n",
    "\n",
    "b7=sig_derivative(0.999)*b4\n",
    "print(b7)\n",
    "\n",
    "b8=sig_derivative(0)*b3\n",
    "print(b8)\n",
    "\n",
    "#Backpropagation - Level 5\n",
    "b9_1=b6*0.5\n",
    "print(b9_1)\n",
    "\n",
    "b9_2=b6*1\n",
    "print(b9_2)\n",
    "\n",
    "b10_1=b7*2\n",
    "print(b10_1)\n",
    "\n",
    "b10_2=b7*10\n",
    "print(b10_2)\n",
    "\n",
    "b11_1=b8*-5\n",
    "print(b11_1)\n",
    "\n",
    "b11_2=b8*-0.3\n",
    "print(b11_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10000000000000009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5700438403325201"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Presentation - Derivative cost function: Step 1\n",
    "print(2*(0.95-1))\n",
    "\n",
    "# Assignment\n",
    "2*(0.71497807983374-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0045176659730912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.11614142888214005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Presentation - Derivative sigmoid: Step 2\n",
    "print(sig_derivative(3)*-0.1)\n",
    "\n",
    "# Assignment\n",
    "sig_derivative(0.92)*-0.57\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 4 (ANN 3)\n",
    "The following questions are drawn from exercise set 4. Once again we provide you the code required to answer the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b0e047cc3247ad8dd31c9129580bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | loss: 832.710205078125 | fraction correct: 0.025\n",
      "10 | loss: 676.6260375976562 | fraction correct: 0.16\n",
      "20 | loss: 675.1060791015625 | fraction correct: 0.155\n",
      "30 | loss: 581.9262084960938 | fraction correct: 0.245\n",
      "40 | loss: 571.442138671875 | fraction correct: 0.23\n",
      "50 | loss: 538.3378295898438 | fraction correct: 0.225\n",
      "60 | loss: 516.1564331054688 | fraction correct: 0.28\n",
      "70 | loss: 482.0628662109375 | fraction correct: 0.335\n",
      "80 | loss: 488.80194091796875 | fraction correct: 0.29\n",
      "90 | loss: 519.5597534179688 | fraction correct: 0.28\n",
      "100 | loss: 485.7309265136719 | fraction correct: 0.34\n",
      "110 | loss: 463.5359191894531 | fraction correct: 0.315\n",
      "120 | loss: 459.64569091796875 | fraction correct: 0.335\n",
      "130 | loss: 510.4762878417969 | fraction correct: 0.305\n",
      "140 | loss: 417.46405029296875 | fraction correct: 0.385\n",
      "150 | loss: 459.86712646484375 | fraction correct: 0.37\n",
      "160 | loss: 480.0617370605469 | fraction correct: 0.3\n",
      "170 | loss: 448.4393310546875 | fraction correct: 0.395\n",
      "180 | loss: 427.5042724609375 | fraction correct: 0.355\n",
      "190 | loss: 447.3929138183594 | fraction correct: 0.335\n",
      "200 | loss: 433.1715087890625 | fraction correct: 0.48\n",
      "210 | loss: 439.3468322753906 | fraction correct: 0.375\n",
      "220 | loss: 416.5601806640625 | fraction correct: 0.405\n",
      "230 | loss: 457.34942626953125 | fraction correct: 0.35\n",
      "240 | loss: 427.9415588378906 | fraction correct: 0.43\n",
      "250 | loss: 403.5128173828125 | fraction correct: 0.445\n",
      "260 | loss: 439.7364501953125 | fraction correct: 0.38\n",
      "270 | loss: 433.4727783203125 | fraction correct: 0.38\n",
      "280 | loss: 364.8638610839844 | fraction correct: 0.52\n",
      "290 | loss: 429.67694091796875 | fraction correct: 0.365\n",
      "300 | loss: 320.209716796875 | fraction correct: 0.54\n",
      "310 | loss: 421.60919189453125 | fraction correct: 0.43\n",
      "320 | loss: 360.213623046875 | fraction correct: 0.485\n",
      "330 | loss: 391.6488037109375 | fraction correct: 0.49\n",
      "340 | loss: 414.1185302734375 | fraction correct: 0.415\n",
      "350 | loss: 394.0334167480469 | fraction correct: 0.43\n",
      "360 | loss: 389.20989990234375 | fraction correct: 0.445\n",
      "370 | loss: 398.30419921875 | fraction correct: 0.445\n",
      "380 | loss: 388.13116455078125 | fraction correct: 0.42\n",
      "390 | loss: 396.2629699707031 | fraction correct: 0.41\n",
      "400 | loss: 414.5977478027344 | fraction correct: 0.42\n",
      "410 | loss: 401.16278076171875 | fraction correct: 0.445\n",
      "420 | loss: 442.5574645996094 | fraction correct: 0.325\n",
      "430 | loss: 409.7242736816406 | fraction correct: 0.41\n",
      "440 | loss: 330.66217041015625 | fraction correct: 0.525\n",
      "450 | loss: 341.0491027832031 | fraction correct: 0.515\n",
      "460 | loss: 423.4705505371094 | fraction correct: 0.36\n",
      "470 | loss: 403.85736083984375 | fraction correct: 0.4\n",
      "480 | loss: 288.9609680175781 | fraction correct: 0.59\n",
      "490 | loss: 331.04327392578125 | fraction correct: 0.5\n",
      "500 | loss: 378.4533996582031 | fraction correct: 0.425\n",
      "510 | loss: 349.9548645019531 | fraction correct: 0.51\n",
      "520 | loss: 439.9230041503906 | fraction correct: 0.365\n",
      "530 | loss: 376.09564208984375 | fraction correct: 0.43\n",
      "540 | loss: 353.8125915527344 | fraction correct: 0.435\n",
      "550 | loss: 422.7202453613281 | fraction correct: 0.415\n",
      "560 | loss: 346.8997802734375 | fraction correct: 0.53\n",
      "570 | loss: 393.6242980957031 | fraction correct: 0.43\n",
      "580 | loss: 312.04656982421875 | fraction correct: 0.565\n",
      "590 | loss: 463.89044189453125 | fraction correct: 0.33\n",
      "600 | loss: 362.90118408203125 | fraction correct: 0.47\n",
      "610 | loss: 401.4143981933594 | fraction correct: 0.42\n",
      "620 | loss: 353.2205505371094 | fraction correct: 0.475\n",
      "630 | loss: 446.0392150878906 | fraction correct: 0.365\n",
      "640 | loss: 357.5194396972656 | fraction correct: 0.425\n",
      "650 | loss: 391.0328063964844 | fraction correct: 0.405\n",
      "660 | loss: 446.9462585449219 | fraction correct: 0.32\n",
      "670 | loss: 399.46807861328125 | fraction correct: 0.405\n",
      "680 | loss: 449.7928771972656 | fraction correct: 0.36\n",
      "690 | loss: 315.49169921875 | fraction correct: 0.515\n",
      "700 | loss: 380.2584228515625 | fraction correct: 0.41\n",
      "710 | loss: 423.99676513671875 | fraction correct: 0.395\n",
      "720 | loss: 376.22314453125 | fraction correct: 0.425\n",
      "730 | loss: 424.1424255371094 | fraction correct: 0.36\n",
      "740 | loss: 316.328125 | fraction correct: 0.54\n",
      "750 | loss: 387.4920959472656 | fraction correct: 0.41\n",
      "760 | loss: 373.45458984375 | fraction correct: 0.475\n",
      "770 | loss: 378.3134460449219 | fraction correct: 0.435\n",
      "780 | loss: 393.3473815917969 | fraction correct: 0.425\n",
      "790 | loss: 293.9854431152344 | fraction correct: 0.575\n",
      "800 | loss: 379.44384765625 | fraction correct: 0.46\n",
      "810 | loss: 340.0718078613281 | fraction correct: 0.485\n",
      "820 | loss: 295.125244140625 | fraction correct: 0.615\n",
      "830 | loss: 355.3669738769531 | fraction correct: 0.51\n",
      "840 | loss: 379.1555480957031 | fraction correct: 0.415\n",
      "850 | loss: 411.2971496582031 | fraction correct: 0.43\n",
      "860 | loss: 369.44830322265625 | fraction correct: 0.455\n",
      "870 | loss: 371.9853820800781 | fraction correct: 0.465\n",
      "880 | loss: 422.3861999511719 | fraction correct: 0.35\n",
      "890 | loss: 329.32135009765625 | fraction correct: 0.49\n",
      "900 | loss: 290.5003662109375 | fraction correct: 0.6\n",
      "910 | loss: 311.69305419921875 | fraction correct: 0.565\n",
      "920 | loss: 278.4117126464844 | fraction correct: 0.595\n",
      "930 | loss: 343.4769592285156 | fraction correct: 0.49\n",
      "940 | loss: 259.43731689453125 | fraction correct: 0.64\n",
      "950 | loss: 379.0694274902344 | fraction correct: 0.405\n",
      "960 | loss: 319.282958984375 | fraction correct: 0.545\n",
      "970 | loss: 360.9841003417969 | fraction correct: 0.46\n",
      "980 | loss: 402.20928955078125 | fraction correct: 0.39\n",
      "990 | loss: 300.5426330566406 | fraction correct: 0.58\n",
      "1000 | loss: 359.67889404296875 | fraction correct: 0.46\n",
      "1010 | loss: 423.6090087890625 | fraction correct: 0.395\n",
      "1020 | loss: 375.40667724609375 | fraction correct: 0.465\n",
      "1030 | loss: 350.9486389160156 | fraction correct: 0.51\n",
      "1040 | loss: 345.7126159667969 | fraction correct: 0.455\n",
      "1050 | loss: 323.0320739746094 | fraction correct: 0.525\n",
      "1060 | loss: 346.1410827636719 | fraction correct: 0.515\n",
      "1070 | loss: 397.490478515625 | fraction correct: 0.425\n",
      "1080 | loss: 289.40283203125 | fraction correct: 0.58\n",
      "1090 | loss: 331.81854248046875 | fraction correct: 0.51\n",
      "1100 | loss: 298.3460998535156 | fraction correct: 0.56\n",
      "1110 | loss: 290.71435546875 | fraction correct: 0.54\n",
      "1120 | loss: 380.5431823730469 | fraction correct: 0.44\n",
      "1130 | loss: 283.4073486328125 | fraction correct: 0.56\n",
      "1140 | loss: 346.7386779785156 | fraction correct: 0.495\n",
      "1150 | loss: 392.1116027832031 | fraction correct: 0.415\n",
      "1160 | loss: 425.718017578125 | fraction correct: 0.37\n",
      "1170 | loss: 296.4815368652344 | fraction correct: 0.56\n",
      "1180 | loss: 348.9991149902344 | fraction correct: 0.495\n",
      "1190 | loss: 374.5375671386719 | fraction correct: 0.445\n",
      "1200 | loss: 331.0495300292969 | fraction correct: 0.545\n",
      "1210 | loss: 417.3067321777344 | fraction correct: 0.34\n",
      "1220 | loss: 330.6083068847656 | fraction correct: 0.555\n",
      "1230 | loss: 390.4714660644531 | fraction correct: 0.44\n",
      "1240 | loss: 356.5638122558594 | fraction correct: 0.48\n",
      "1250 | loss: 350.4124450683594 | fraction correct: 0.475\n",
      "1260 | loss: 386.1622619628906 | fraction correct: 0.41\n",
      "1270 | loss: 417.1744384765625 | fraction correct: 0.415\n",
      "1280 | loss: 433.421142578125 | fraction correct: 0.35\n",
      "1290 | loss: 367.3665771484375 | fraction correct: 0.455\n",
      "1300 | loss: 374.5420837402344 | fraction correct: 0.38\n",
      "1310 | loss: 354.593017578125 | fraction correct: 0.45\n",
      "1320 | loss: 353.0906066894531 | fraction correct: 0.47\n",
      "1330 | loss: 398.51190185546875 | fraction correct: 0.435\n",
      "1340 | loss: 391.903564453125 | fraction correct: 0.435\n",
      "1350 | loss: 370.60693359375 | fraction correct: 0.45\n",
      "1360 | loss: 327.67999267578125 | fraction correct: 0.53\n",
      "1370 | loss: 349.1643981933594 | fraction correct: 0.495\n",
      "1380 | loss: 392.8172607421875 | fraction correct: 0.43\n",
      "1390 | loss: 335.270263671875 | fraction correct: 0.505\n",
      "1400 | loss: 305.3204345703125 | fraction correct: 0.585\n",
      "1410 | loss: 227.84286499023438 | fraction correct: 0.67\n",
      "1420 | loss: 373.43023681640625 | fraction correct: 0.45\n",
      "1430 | loss: 260.9261474609375 | fraction correct: 0.63\n",
      "1440 | loss: 364.87896728515625 | fraction correct: 0.45\n",
      "1450 | loss: 404.7193603515625 | fraction correct: 0.405\n",
      "1460 | loss: 338.94610595703125 | fraction correct: 0.515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470 | loss: 365.1872863769531 | fraction correct: 0.47\n",
      "1480 | loss: 384.2205810546875 | fraction correct: 0.485\n",
      "1490 | loss: 322.62371826171875 | fraction correct: 0.595\n",
      "1500 | loss: 338.78582763671875 | fraction correct: 0.465\n",
      "1510 | loss: 328.0692138671875 | fraction correct: 0.5\n",
      "1520 | loss: 337.9828796386719 | fraction correct: 0.5\n",
      "1530 | loss: 313.8789367675781 | fraction correct: 0.515\n",
      "1540 | loss: 309.4690246582031 | fraction correct: 0.545\n",
      "1550 | loss: 320.9605407714844 | fraction correct: 0.545\n",
      "1560 | loss: 357.3541564941406 | fraction correct: 0.45\n",
      "1570 | loss: 380.58074951171875 | fraction correct: 0.43\n",
      "1580 | loss: 347.4920959472656 | fraction correct: 0.52\n",
      "1590 | loss: 293.1967468261719 | fraction correct: 0.55\n",
      "1600 | loss: 343.212646484375 | fraction correct: 0.44\n",
      "1610 | loss: 373.9688415527344 | fraction correct: 0.455\n",
      "1620 | loss: 311.2682800292969 | fraction correct: 0.54\n",
      "1630 | loss: 294.8973083496094 | fraction correct: 0.54\n",
      "1640 | loss: 382.2886657714844 | fraction correct: 0.415\n",
      "1650 | loss: 273.0264587402344 | fraction correct: 0.585\n",
      "1660 | loss: 337.2938232421875 | fraction correct: 0.545\n",
      "1670 | loss: 304.94921875 | fraction correct: 0.57\n",
      "1680 | loss: 351.7817687988281 | fraction correct: 0.48\n",
      "1690 | loss: 374.9915466308594 | fraction correct: 0.405\n",
      "1700 | loss: 345.9904479980469 | fraction correct: 0.46\n",
      "1710 | loss: 330.57330322265625 | fraction correct: 0.5\n",
      "1720 | loss: 359.8108825683594 | fraction correct: 0.45\n",
      "1730 | loss: 333.9261169433594 | fraction correct: 0.475\n",
      "1740 | loss: 322.6995544433594 | fraction correct: 0.515\n",
      "1750 | loss: 303.383544921875 | fraction correct: 0.565\n",
      "1760 | loss: 297.99029541015625 | fraction correct: 0.53\n",
      "1770 | loss: 371.3857116699219 | fraction correct: 0.435\n",
      "1780 | loss: 332.1764831542969 | fraction correct: 0.47\n",
      "1790 | loss: 277.79248046875 | fraction correct: 0.605\n",
      "1800 | loss: 312.9364013671875 | fraction correct: 0.575\n",
      "1810 | loss: 301.4112548828125 | fraction correct: 0.525\n",
      "1820 | loss: 290.290771484375 | fraction correct: 0.585\n",
      "1830 | loss: 322.2217102050781 | fraction correct: 0.485\n",
      "1840 | loss: 388.5610046386719 | fraction correct: 0.425\n",
      "1850 | loss: 341.73779296875 | fraction correct: 0.53\n",
      "1860 | loss: 358.1841735839844 | fraction correct: 0.49\n",
      "1870 | loss: 381.29547119140625 | fraction correct: 0.43\n",
      "1880 | loss: 264.13372802734375 | fraction correct: 0.61\n",
      "1890 | loss: 307.8901062011719 | fraction correct: 0.555\n",
      "1900 | loss: 318.02923583984375 | fraction correct: 0.505\n",
      "1910 | loss: 346.4066162109375 | fraction correct: 0.49\n",
      "1920 | loss: 288.2034606933594 | fraction correct: 0.57\n",
      "1930 | loss: 361.4466552734375 | fraction correct: 0.44\n",
      "1940 | loss: 397.6922302246094 | fraction correct: 0.4\n",
      "1950 | loss: 297.29437255859375 | fraction correct: 0.535\n",
      "1960 | loss: 390.7075500488281 | fraction correct: 0.43\n",
      "1970 | loss: 365.4685974121094 | fraction correct: 0.44\n",
      "1980 | loss: 289.0921630859375 | fraction correct: 0.57\n",
      "1990 | loss: 257.1186218261719 | fraction correct: 0.62\n",
      "2000 | loss: 386.641845703125 | fraction correct: 0.45\n",
      "2010 | loss: 367.9327392578125 | fraction correct: 0.465\n",
      "2020 | loss: 256.0973815917969 | fraction correct: 0.645\n",
      "2030 | loss: 383.5096435546875 | fraction correct: 0.45\n",
      "2040 | loss: 330.6457214355469 | fraction correct: 0.485\n",
      "2050 | loss: 346.629638671875 | fraction correct: 0.49\n",
      "2060 | loss: 427.2383728027344 | fraction correct: 0.39\n",
      "2070 | loss: 241.19830322265625 | fraction correct: 0.66\n",
      "2080 | loss: 354.89764404296875 | fraction correct: 0.49\n",
      "2090 | loss: 282.6745910644531 | fraction correct: 0.595\n",
      "2100 | loss: 398.7868347167969 | fraction correct: 0.38\n",
      "2110 | loss: 303.7896728515625 | fraction correct: 0.535\n",
      "2120 | loss: 305.63165283203125 | fraction correct: 0.56\n",
      "2130 | loss: 391.08221435546875 | fraction correct: 0.44\n",
      "2140 | loss: 413.4918212890625 | fraction correct: 0.4\n",
      "2150 | loss: 337.1092834472656 | fraction correct: 0.525\n",
      "2160 | loss: 330.5185852050781 | fraction correct: 0.49\n",
      "2170 | loss: 292.79730224609375 | fraction correct: 0.5\n",
      "2180 | loss: 322.1800842285156 | fraction correct: 0.53\n",
      "2190 | loss: 340.4067687988281 | fraction correct: 0.545\n",
      "2200 | loss: 282.1123352050781 | fraction correct: 0.59\n",
      "2210 | loss: 307.1943359375 | fraction correct: 0.535\n",
      "2220 | loss: 322.30145263671875 | fraction correct: 0.51\n",
      "2230 | loss: 312.7628479003906 | fraction correct: 0.525\n",
      "2240 | loss: 320.6798095703125 | fraction correct: 0.52\n",
      "2250 | loss: 268.04742431640625 | fraction correct: 0.63\n",
      "2260 | loss: 260.59918212890625 | fraction correct: 0.645\n",
      "2270 | loss: 315.88037109375 | fraction correct: 0.53\n",
      "2280 | loss: 376.9883728027344 | fraction correct: 0.445\n",
      "2290 | loss: 280.91259765625 | fraction correct: 0.585\n",
      "2300 | loss: 323.90625 | fraction correct: 0.48\n",
      "2310 | loss: 325.98211669921875 | fraction correct: 0.52\n",
      "2320 | loss: 341.69000244140625 | fraction correct: 0.48\n",
      "2330 | loss: 303.1234436035156 | fraction correct: 0.555\n",
      "2340 | loss: 312.7733154296875 | fraction correct: 0.52\n",
      "2350 | loss: 290.1874694824219 | fraction correct: 0.58\n",
      "2360 | loss: 320.20318603515625 | fraction correct: 0.51\n",
      "2370 | loss: 339.18975830078125 | fraction correct: 0.46\n",
      "2380 | loss: 407.658935546875 | fraction correct: 0.41\n",
      "2390 | loss: 354.39508056640625 | fraction correct: 0.545\n",
      "2400 | loss: 384.5391845703125 | fraction correct: 0.425\n",
      "2410 | loss: 330.1502380371094 | fraction correct: 0.495\n",
      "2420 | loss: 333.4329833984375 | fraction correct: 0.515\n",
      "2430 | loss: 293.5058288574219 | fraction correct: 0.62\n",
      "2440 | loss: 353.7159729003906 | fraction correct: 0.49\n",
      "2450 | loss: 349.16265869140625 | fraction correct: 0.435\n",
      "2460 | loss: 336.74407958984375 | fraction correct: 0.49\n",
      "2470 | loss: 308.8564147949219 | fraction correct: 0.51\n",
      "2480 | loss: 354.9619140625 | fraction correct: 0.49\n",
      "2490 | loss: 313.3516540527344 | fraction correct: 0.56\n",
      "2500 | loss: 341.5972595214844 | fraction correct: 0.46\n",
      "2510 | loss: 277.3697204589844 | fraction correct: 0.62\n",
      "2520 | loss: 329.4157409667969 | fraction correct: 0.54\n",
      "2530 | loss: 338.4735107421875 | fraction correct: 0.46\n",
      "2540 | loss: 345.3644714355469 | fraction correct: 0.51\n",
      "2550 | loss: 300.1736755371094 | fraction correct: 0.55\n",
      "2560 | loss: 308.81793212890625 | fraction correct: 0.575\n",
      "2570 | loss: 327.0110778808594 | fraction correct: 0.505\n",
      "2580 | loss: 291.32415771484375 | fraction correct: 0.645\n",
      "2590 | loss: 303.1795349121094 | fraction correct: 0.525\n",
      "2600 | loss: 326.192626953125 | fraction correct: 0.515\n",
      "2610 | loss: 313.0781555175781 | fraction correct: 0.545\n",
      "2620 | loss: 297.5700988769531 | fraction correct: 0.555\n",
      "2630 | loss: 337.3951110839844 | fraction correct: 0.495\n",
      "2640 | loss: 356.1117248535156 | fraction correct: 0.46\n",
      "2650 | loss: 288.27825927734375 | fraction correct: 0.57\n",
      "2660 | loss: 221.9489288330078 | fraction correct: 0.69\n",
      "2670 | loss: 280.1873474121094 | fraction correct: 0.565\n",
      "2680 | loss: 362.9368591308594 | fraction correct: 0.45\n",
      "2690 | loss: 316.9150085449219 | fraction correct: 0.515\n",
      "2700 | loss: 314.6891174316406 | fraction correct: 0.54\n",
      "2710 | loss: 213.78680419921875 | fraction correct: 0.66\n",
      "2720 | loss: 380.91583251953125 | fraction correct: 0.37\n",
      "2730 | loss: 317.2820129394531 | fraction correct: 0.59\n",
      "2740 | loss: 326.8766174316406 | fraction correct: 0.485\n",
      "2750 | loss: 374.4016418457031 | fraction correct: 0.485\n",
      "2760 | loss: 300.3617858886719 | fraction correct: 0.5\n",
      "2770 | loss: 374.8021545410156 | fraction correct: 0.47\n",
      "2780 | loss: 309.7438659667969 | fraction correct: 0.525\n",
      "2790 | loss: 310.60040283203125 | fraction correct: 0.55\n",
      "2800 | loss: 308.6116027832031 | fraction correct: 0.56\n",
      "2810 | loss: 362.970703125 | fraction correct: 0.45\n",
      "2820 | loss: 359.56402587890625 | fraction correct: 0.475\n",
      "2830 | loss: 326.4254455566406 | fraction correct: 0.53\n",
      "2840 | loss: 365.1620788574219 | fraction correct: 0.42\n",
      "2850 | loss: 360.14013671875 | fraction correct: 0.46\n",
      "2860 | loss: 321.99871826171875 | fraction correct: 0.56\n",
      "2870 | loss: 321.0007629394531 | fraction correct: 0.495\n",
      "2880 | loss: 413.7244873046875 | fraction correct: 0.4\n",
      "2890 | loss: 319.7810363769531 | fraction correct: 0.535\n",
      "2900 | loss: 325.51422119140625 | fraction correct: 0.49\n",
      "2910 | loss: 393.8260498046875 | fraction correct: 0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920 | loss: 335.1889953613281 | fraction correct: 0.515\n",
      "2930 | loss: 267.15869140625 | fraction correct: 0.56\n",
      "2940 | loss: 240.1629638671875 | fraction correct: 0.645\n",
      "2950 | loss: 310.4151611328125 | fraction correct: 0.535\n",
      "2960 | loss: 371.5245361328125 | fraction correct: 0.455\n",
      "2970 | loss: 382.43682861328125 | fraction correct: 0.37\n",
      "2980 | loss: 355.5504455566406 | fraction correct: 0.505\n",
      "2990 | loss: 257.58056640625 | fraction correct: 0.63\n",
      "3000 | loss: 372.7867126464844 | fraction correct: 0.425\n",
      "3010 | loss: 314.697998046875 | fraction correct: 0.515\n",
      "3020 | loss: 321.51165771484375 | fraction correct: 0.545\n",
      "3030 | loss: 340.4962463378906 | fraction correct: 0.505\n",
      "3040 | loss: 376.33233642578125 | fraction correct: 0.47\n",
      "3050 | loss: 384.1531982421875 | fraction correct: 0.43\n",
      "3060 | loss: 355.665771484375 | fraction correct: 0.44\n",
      "3070 | loss: 333.4940185546875 | fraction correct: 0.48\n",
      "3080 | loss: 337.3923034667969 | fraction correct: 0.475\n",
      "3090 | loss: 247.85159301757812 | fraction correct: 0.63\n",
      "3100 | loss: 260.3612060546875 | fraction correct: 0.605\n",
      "3110 | loss: 325.3477478027344 | fraction correct: 0.465\n",
      "3120 | loss: 293.2501220703125 | fraction correct: 0.58\n",
      "3130 | loss: 320.4578552246094 | fraction correct: 0.545\n",
      "3140 | loss: 329.2628173828125 | fraction correct: 0.5\n",
      "3150 | loss: 417.4685363769531 | fraction correct: 0.39\n",
      "3160 | loss: 344.2483215332031 | fraction correct: 0.49\n",
      "3170 | loss: 322.1587829589844 | fraction correct: 0.515\n",
      "3180 | loss: 330.8686828613281 | fraction correct: 0.475\n",
      "3190 | loss: 277.4541320800781 | fraction correct: 0.63\n",
      "3200 | loss: 288.1806640625 | fraction correct: 0.565\n",
      "3210 | loss: 278.689697265625 | fraction correct: 0.575\n",
      "3220 | loss: 345.3912353515625 | fraction correct: 0.47\n",
      "3230 | loss: 297.6407775878906 | fraction correct: 0.535\n",
      "3240 | loss: 253.72409057617188 | fraction correct: 0.695\n",
      "3250 | loss: 346.5902099609375 | fraction correct: 0.505\n",
      "3260 | loss: 294.8839111328125 | fraction correct: 0.605\n",
      "3270 | loss: 289.47052001953125 | fraction correct: 0.6\n",
      "3280 | loss: 347.0177001953125 | fraction correct: 0.46\n",
      "3290 | loss: 374.44866943359375 | fraction correct: 0.435\n",
      "3300 | loss: 299.648193359375 | fraction correct: 0.55\n",
      "3310 | loss: 299.8450927734375 | fraction correct: 0.565\n",
      "3320 | loss: 343.44500732421875 | fraction correct: 0.52\n",
      "3330 | loss: 284.3338317871094 | fraction correct: 0.555\n",
      "3340 | loss: 345.9674377441406 | fraction correct: 0.44\n",
      "3350 | loss: 286.3565979003906 | fraction correct: 0.51\n",
      "3360 | loss: 328.5902099609375 | fraction correct: 0.485\n",
      "3370 | loss: 225.9060516357422 | fraction correct: 0.645\n",
      "3380 | loss: 379.9228515625 | fraction correct: 0.455\n",
      "3390 | loss: 323.90985107421875 | fraction correct: 0.53\n",
      "3400 | loss: 258.8520812988281 | fraction correct: 0.615\n",
      "3410 | loss: 258.6354675292969 | fraction correct: 0.61\n",
      "3420 | loss: 319.8515930175781 | fraction correct: 0.49\n",
      "3430 | loss: 357.48260498046875 | fraction correct: 0.455\n",
      "3440 | loss: 323.3450622558594 | fraction correct: 0.545\n",
      "3450 | loss: 318.4271545410156 | fraction correct: 0.535\n",
      "3460 | loss: 334.8153381347656 | fraction correct: 0.49\n",
      "3470 | loss: 300.97064208984375 | fraction correct: 0.525\n",
      "3480 | loss: 305.5501403808594 | fraction correct: 0.585\n",
      "3490 | loss: 382.8016357421875 | fraction correct: 0.425\n",
      "3500 | loss: 243.57432556152344 | fraction correct: 0.645\n",
      "3510 | loss: 381.365478515625 | fraction correct: 0.4\n",
      "3520 | loss: 348.2253723144531 | fraction correct: 0.5\n",
      "3530 | loss: 347.3674621582031 | fraction correct: 0.48\n",
      "3540 | loss: 349.86578369140625 | fraction correct: 0.49\n",
      "3550 | loss: 303.7207946777344 | fraction correct: 0.555\n",
      "3560 | loss: 342.3868713378906 | fraction correct: 0.475\n",
      "3570 | loss: 363.2470397949219 | fraction correct: 0.44\n",
      "3580 | loss: 338.2225646972656 | fraction correct: 0.5\n",
      "3590 | loss: 365.7366027832031 | fraction correct: 0.435\n",
      "3600 | loss: 352.6387634277344 | fraction correct: 0.415\n",
      "3610 | loss: 347.6424865722656 | fraction correct: 0.435\n",
      "3620 | loss: 313.3849182128906 | fraction correct: 0.5\n",
      "3630 | loss: 365.42864990234375 | fraction correct: 0.44\n",
      "3640 | loss: 343.31494140625 | fraction correct: 0.51\n",
      "3650 | loss: 388.6817626953125 | fraction correct: 0.425\n",
      "3660 | loss: 311.05389404296875 | fraction correct: 0.57\n",
      "3670 | loss: 288.7303771972656 | fraction correct: 0.555\n",
      "3680 | loss: 329.6667785644531 | fraction correct: 0.545\n",
      "3690 | loss: 386.7745666503906 | fraction correct: 0.4\n",
      "3700 | loss: 316.11322021484375 | fraction correct: 0.53\n",
      "3710 | loss: 287.2499084472656 | fraction correct: 0.585\n",
      "3720 | loss: 226.52536010742188 | fraction correct: 0.66\n",
      "3730 | loss: 274.1348571777344 | fraction correct: 0.56\n",
      "3740 | loss: 317.32330322265625 | fraction correct: 0.505\n",
      "3750 | loss: 369.3127136230469 | fraction correct: 0.44\n",
      "3760 | loss: 262.5816650390625 | fraction correct: 0.625\n",
      "3770 | loss: 401.62298583984375 | fraction correct: 0.425\n",
      "3780 | loss: 347.87109375 | fraction correct: 0.48\n",
      "3790 | loss: 356.27166748046875 | fraction correct: 0.535\n",
      "3800 | loss: 199.69296264648438 | fraction correct: 0.685\n",
      "3810 | loss: 327.6700744628906 | fraction correct: 0.46\n",
      "3820 | loss: 291.6175842285156 | fraction correct: 0.58\n",
      "3830 | loss: 325.77947998046875 | fraction correct: 0.495\n",
      "3840 | loss: 305.5991516113281 | fraction correct: 0.55\n",
      "3850 | loss: 267.54840087890625 | fraction correct: 0.585\n",
      "3860 | loss: 300.4572448730469 | fraction correct: 0.58\n",
      "3870 | loss: 350.02484130859375 | fraction correct: 0.505\n",
      "3880 | loss: 306.55609130859375 | fraction correct: 0.535\n",
      "3890 | loss: 330.39801025390625 | fraction correct: 0.535\n",
      "3900 | loss: 363.642333984375 | fraction correct: 0.455\n",
      "3910 | loss: 326.6325988769531 | fraction correct: 0.505\n",
      "3920 | loss: 270.6949462890625 | fraction correct: 0.605\n",
      "3930 | loss: 329.32281494140625 | fraction correct: 0.505\n",
      "3940 | loss: 318.3761901855469 | fraction correct: 0.5\n",
      "3950 | loss: 333.215087890625 | fraction correct: 0.515\n",
      "3960 | loss: 348.1885986328125 | fraction correct: 0.495\n",
      "3970 | loss: 321.3752746582031 | fraction correct: 0.49\n",
      "3980 | loss: 380.0531921386719 | fraction correct: 0.48\n",
      "3990 | loss: 327.6563720703125 | fraction correct: 0.495\n",
      "4000 | loss: 285.068603515625 | fraction correct: 0.55\n",
      "4010 | loss: 354.94940185546875 | fraction correct: 0.475\n",
      "4020 | loss: 350.5186462402344 | fraction correct: 0.51\n",
      "4030 | loss: 309.4375915527344 | fraction correct: 0.55\n",
      "4040 | loss: 332.460205078125 | fraction correct: 0.485\n",
      "4050 | loss: 374.2485046386719 | fraction correct: 0.415\n",
      "4060 | loss: 298.3390197753906 | fraction correct: 0.53\n",
      "4070 | loss: 356.9320983886719 | fraction correct: 0.44\n",
      "4080 | loss: 311.04278564453125 | fraction correct: 0.505\n",
      "4090 | loss: 366.32183837890625 | fraction correct: 0.425\n",
      "4100 | loss: 283.3742980957031 | fraction correct: 0.625\n",
      "4110 | loss: 375.0610046386719 | fraction correct: 0.425\n",
      "4120 | loss: 388.5743713378906 | fraction correct: 0.415\n",
      "4130 | loss: 292.8387451171875 | fraction correct: 0.53\n",
      "4140 | loss: 360.2133483886719 | fraction correct: 0.45\n",
      "4150 | loss: 288.5785217285156 | fraction correct: 0.595\n",
      "4160 | loss: 268.4505615234375 | fraction correct: 0.55\n",
      "4170 | loss: 308.24603271484375 | fraction correct: 0.545\n",
      "4180 | loss: 341.3443298339844 | fraction correct: 0.525\n",
      "4190 | loss: 265.73883056640625 | fraction correct: 0.6\n",
      "4200 | loss: 334.7720947265625 | fraction correct: 0.48\n",
      "4210 | loss: 233.15155029296875 | fraction correct: 0.66\n",
      "4220 | loss: 325.262451171875 | fraction correct: 0.515\n",
      "4230 | loss: 325.0958557128906 | fraction correct: 0.485\n",
      "4240 | loss: 378.32916259765625 | fraction correct: 0.445\n",
      "4250 | loss: 350.5154724121094 | fraction correct: 0.475\n",
      "4260 | loss: 313.1483459472656 | fraction correct: 0.535\n",
      "4270 | loss: 387.0107116699219 | fraction correct: 0.46\n",
      "4280 | loss: 384.2729797363281 | fraction correct: 0.445\n",
      "4290 | loss: 380.4939270019531 | fraction correct: 0.44\n",
      "4300 | loss: 348.71978759765625 | fraction correct: 0.49\n",
      "4310 | loss: 326.23699951171875 | fraction correct: 0.51\n",
      "4320 | loss: 395.7756652832031 | fraction correct: 0.445\n",
      "4330 | loss: 288.962646484375 | fraction correct: 0.58\n",
      "4340 | loss: 373.0464782714844 | fraction correct: 0.46\n",
      "4350 | loss: 268.21063232421875 | fraction correct: 0.655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360 | loss: 355.7021179199219 | fraction correct: 0.48\n",
      "4370 | loss: 296.4777526855469 | fraction correct: 0.575\n",
      "4380 | loss: 396.0286865234375 | fraction correct: 0.395\n",
      "4390 | loss: 297.6802673339844 | fraction correct: 0.63\n",
      "4400 | loss: 344.0008850097656 | fraction correct: 0.47\n",
      "4410 | loss: 358.5809326171875 | fraction correct: 0.48\n",
      "4420 | loss: 328.6910095214844 | fraction correct: 0.45\n",
      "4430 | loss: 213.67776489257812 | fraction correct: 0.635\n",
      "4440 | loss: 266.8737487792969 | fraction correct: 0.63\n",
      "4450 | loss: 287.68597412109375 | fraction correct: 0.57\n",
      "4460 | loss: 272.41400146484375 | fraction correct: 0.625\n",
      "4470 | loss: 344.5796813964844 | fraction correct: 0.455\n",
      "4480 | loss: 265.618896484375 | fraction correct: 0.61\n",
      "4490 | loss: 294.9054260253906 | fraction correct: 0.52\n",
      "4500 | loss: 302.33587646484375 | fraction correct: 0.55\n",
      "4510 | loss: 333.74072265625 | fraction correct: 0.505\n",
      "4520 | loss: 410.7789611816406 | fraction correct: 0.455\n",
      "4530 | loss: 290.0805969238281 | fraction correct: 0.56\n",
      "4540 | loss: 391.97027587890625 | fraction correct: 0.435\n",
      "4550 | loss: 290.239013671875 | fraction correct: 0.54\n",
      "4560 | loss: 343.2033386230469 | fraction correct: 0.545\n",
      "4570 | loss: 334.5948486328125 | fraction correct: 0.52\n",
      "4580 | loss: 256.05810546875 | fraction correct: 0.62\n",
      "4590 | loss: 316.3009948730469 | fraction correct: 0.505\n",
      "4600 | loss: 353.8026123046875 | fraction correct: 0.465\n",
      "4610 | loss: 365.1334533691406 | fraction correct: 0.42\n",
      "4620 | loss: 266.1941223144531 | fraction correct: 0.59\n",
      "4630 | loss: 285.4930725097656 | fraction correct: 0.545\n",
      "4640 | loss: 275.3659362792969 | fraction correct: 0.56\n",
      "4650 | loss: 366.8663024902344 | fraction correct: 0.435\n",
      "4660 | loss: 208.439208984375 | fraction correct: 0.69\n",
      "4670 | loss: 307.3902282714844 | fraction correct: 0.55\n",
      "4680 | loss: 292.44049072265625 | fraction correct: 0.6\n",
      "4690 | loss: 375.76202392578125 | fraction correct: 0.455\n",
      "4700 | loss: 323.75897216796875 | fraction correct: 0.56\n",
      "4710 | loss: 332.2425537109375 | fraction correct: 0.495\n",
      "4720 | loss: 333.5303039550781 | fraction correct: 0.48\n",
      "4730 | loss: 354.6583251953125 | fraction correct: 0.48\n",
      "4740 | loss: 266.47119140625 | fraction correct: 0.555\n",
      "4750 | loss: 391.3266296386719 | fraction correct: 0.42\n",
      "4760 | loss: 300.2530212402344 | fraction correct: 0.57\n",
      "4770 | loss: 343.9789123535156 | fraction correct: 0.445\n",
      "4780 | loss: 357.0964050292969 | fraction correct: 0.47\n",
      "4790 | loss: 206.96090698242188 | fraction correct: 0.73\n",
      "4800 | loss: 290.248779296875 | fraction correct: 0.53\n",
      "4810 | loss: 336.4599304199219 | fraction correct: 0.525\n",
      "4820 | loss: 339.04937744140625 | fraction correct: 0.48\n",
      "4830 | loss: 288.68902587890625 | fraction correct: 0.56\n",
      "4840 | loss: 373.9673767089844 | fraction correct: 0.425\n",
      "4850 | loss: 207.13287353515625 | fraction correct: 0.695\n",
      "4860 | loss: 266.3409423828125 | fraction correct: 0.615\n",
      "4870 | loss: 323.46734619140625 | fraction correct: 0.52\n",
      "4880 | loss: 353.3288879394531 | fraction correct: 0.47\n",
      "4890 | loss: 308.17901611328125 | fraction correct: 0.515\n",
      "4900 | loss: 313.5696716308594 | fraction correct: 0.55\n",
      "4910 | loss: 284.5807800292969 | fraction correct: 0.57\n",
      "4920 | loss: 287.365478515625 | fraction correct: 0.57\n",
      "4930 | loss: 278.13275146484375 | fraction correct: 0.575\n",
      "4940 | loss: 346.1955871582031 | fraction correct: 0.42\n",
      "4950 | loss: 326.4538269042969 | fraction correct: 0.55\n",
      "4960 | loss: 402.8177490234375 | fraction correct: 0.435\n",
      "4970 | loss: 293.8848571777344 | fraction correct: 0.535\n",
      "4980 | loss: 250.25494384765625 | fraction correct: 0.61\n",
      "4990 | loss: 307.4759216308594 | fraction correct: 0.53\n",
      "5000 | loss: 253.04429626464844 | fraction correct: 0.65\n",
      "5010 | loss: 328.6878967285156 | fraction correct: 0.495\n",
      "5020 | loss: 348.0307312011719 | fraction correct: 0.505\n",
      "5030 | loss: 314.3709716796875 | fraction correct: 0.53\n",
      "5040 | loss: 311.5386657714844 | fraction correct: 0.53\n",
      "5050 | loss: 398.62200927734375 | fraction correct: 0.435\n",
      "5060 | loss: 289.94476318359375 | fraction correct: 0.54\n",
      "5070 | loss: 315.7915344238281 | fraction correct: 0.49\n",
      "5080 | loss: 250.8215789794922 | fraction correct: 0.615\n",
      "5090 | loss: 340.6239318847656 | fraction correct: 0.52\n",
      "5100 | loss: 236.6068572998047 | fraction correct: 0.625\n",
      "5110 | loss: 255.55735778808594 | fraction correct: 0.63\n",
      "5120 | loss: 335.8042907714844 | fraction correct: 0.505\n",
      "5130 | loss: 262.63739013671875 | fraction correct: 0.615\n",
      "5140 | loss: 358.5156555175781 | fraction correct: 0.45\n",
      "5150 | loss: 270.3453063964844 | fraction correct: 0.61\n",
      "5160 | loss: 263.90814208984375 | fraction correct: 0.565\n",
      "5170 | loss: 275.3122253417969 | fraction correct: 0.63\n",
      "5180 | loss: 288.86895751953125 | fraction correct: 0.595\n",
      "5190 | loss: 359.6831970214844 | fraction correct: 0.475\n",
      "5200 | loss: 282.619384765625 | fraction correct: 0.595\n",
      "5210 | loss: 277.1932067871094 | fraction correct: 0.575\n",
      "5220 | loss: 324.9803771972656 | fraction correct: 0.485\n",
      "5230 | loss: 330.864501953125 | fraction correct: 0.485\n",
      "5240 | loss: 362.0935974121094 | fraction correct: 0.44\n",
      "5250 | loss: 325.01263427734375 | fraction correct: 0.515\n",
      "5260 | loss: 347.929443359375 | fraction correct: 0.445\n",
      "5270 | loss: 283.512939453125 | fraction correct: 0.545\n",
      "5280 | loss: 322.5548400878906 | fraction correct: 0.505\n",
      "5290 | loss: 316.3160705566406 | fraction correct: 0.53\n",
      "5300 | loss: 361.7864685058594 | fraction correct: 0.46\n",
      "5310 | loss: 300.525634765625 | fraction correct: 0.54\n",
      "5320 | loss: 342.7510681152344 | fraction correct: 0.475\n",
      "5330 | loss: 283.2248840332031 | fraction correct: 0.535\n",
      "5340 | loss: 348.9457702636719 | fraction correct: 0.455\n",
      "5350 | loss: 272.9029235839844 | fraction correct: 0.6\n",
      "5360 | loss: 368.0764465332031 | fraction correct: 0.485\n",
      "5370 | loss: 372.7144775390625 | fraction correct: 0.4\n",
      "5380 | loss: 231.7367706298828 | fraction correct: 0.66\n",
      "5390 | loss: 345.134033203125 | fraction correct: 0.48\n",
      "5400 | loss: 310.2730712890625 | fraction correct: 0.52\n",
      "5410 | loss: 373.1307678222656 | fraction correct: 0.43\n",
      "5420 | loss: 270.7302551269531 | fraction correct: 0.575\n",
      "5430 | loss: 235.75636291503906 | fraction correct: 0.7\n",
      "5440 | loss: 320.1202087402344 | fraction correct: 0.51\n",
      "5450 | loss: 323.00469970703125 | fraction correct: 0.435\n",
      "5460 | loss: 305.5318908691406 | fraction correct: 0.55\n",
      "5470 | loss: 228.0967254638672 | fraction correct: 0.625\n",
      "5480 | loss: 284.8341064453125 | fraction correct: 0.57\n",
      "5490 | loss: 356.6056823730469 | fraction correct: 0.48\n",
      "5500 | loss: 299.7046813964844 | fraction correct: 0.515\n",
      "5510 | loss: 236.15081787109375 | fraction correct: 0.67\n",
      "5520 | loss: 363.6936950683594 | fraction correct: 0.45\n",
      "5530 | loss: 311.0798645019531 | fraction correct: 0.55\n",
      "5540 | loss: 229.15907287597656 | fraction correct: 0.67\n",
      "5550 | loss: 260.6880187988281 | fraction correct: 0.595\n",
      "5560 | loss: 346.8301696777344 | fraction correct: 0.465\n",
      "5570 | loss: 292.3868713378906 | fraction correct: 0.555\n",
      "5580 | loss: 250.61936950683594 | fraction correct: 0.59\n",
      "5590 | loss: 251.7880401611328 | fraction correct: 0.625\n",
      "5600 | loss: 288.3342590332031 | fraction correct: 0.525\n",
      "5610 | loss: 333.1430969238281 | fraction correct: 0.455\n",
      "5620 | loss: 196.4390106201172 | fraction correct: 0.685\n",
      "5630 | loss: 367.5970153808594 | fraction correct: 0.505\n",
      "5640 | loss: 252.43043518066406 | fraction correct: 0.59\n",
      "5650 | loss: 304.2181091308594 | fraction correct: 0.49\n",
      "5660 | loss: 227.15081787109375 | fraction correct: 0.64\n",
      "5670 | loss: 335.2640380859375 | fraction correct: 0.505\n",
      "5680 | loss: 270.1441955566406 | fraction correct: 0.595\n",
      "5690 | loss: 293.0021057128906 | fraction correct: 0.585\n",
      "5700 | loss: 277.6221618652344 | fraction correct: 0.645\n",
      "5710 | loss: 319.2049255371094 | fraction correct: 0.52\n",
      "5720 | loss: 289.2926940917969 | fraction correct: 0.6\n",
      "5730 | loss: 324.823486328125 | fraction correct: 0.535\n",
      "5740 | loss: 390.56512451171875 | fraction correct: 0.39\n",
      "5750 | loss: 324.63348388671875 | fraction correct: 0.54\n",
      "5760 | loss: 297.6881103515625 | fraction correct: 0.585\n",
      "5770 | loss: 246.16522216796875 | fraction correct: 0.635\n",
      "5780 | loss: 282.5378723144531 | fraction correct: 0.56\n",
      "5790 | loss: 331.0987548828125 | fraction correct: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5800 | loss: 352.3343811035156 | fraction correct: 0.445\n",
      "5810 | loss: 266.7471008300781 | fraction correct: 0.575\n",
      "5820 | loss: 335.54339599609375 | fraction correct: 0.515\n",
      "5830 | loss: 277.6788024902344 | fraction correct: 0.55\n",
      "5840 | loss: 342.7104187011719 | fraction correct: 0.505\n",
      "5850 | loss: 333.2460021972656 | fraction correct: 0.47\n",
      "5860 | loss: 257.0743713378906 | fraction correct: 0.6\n",
      "5870 | loss: 295.39337158203125 | fraction correct: 0.51\n",
      "5880 | loss: 183.19879150390625 | fraction correct: 0.73\n",
      "5890 | loss: 283.39410400390625 | fraction correct: 0.595\n",
      "5900 | loss: 245.9786834716797 | fraction correct: 0.65\n",
      "5910 | loss: 274.4006042480469 | fraction correct: 0.58\n",
      "5920 | loss: 279.2580261230469 | fraction correct: 0.57\n",
      "5930 | loss: 270.34405517578125 | fraction correct: 0.595\n",
      "5940 | loss: 268.0518798828125 | fraction correct: 0.635\n",
      "5950 | loss: 320.36785888671875 | fraction correct: 0.53\n",
      "5960 | loss: 341.85791015625 | fraction correct: 0.48\n",
      "5970 | loss: 336.8056335449219 | fraction correct: 0.51\n",
      "5980 | loss: 345.6524658203125 | fraction correct: 0.48\n",
      "5990 | loss: 317.2866516113281 | fraction correct: 0.515\n",
      "6000 | loss: 297.09417724609375 | fraction correct: 0.59\n",
      "6010 | loss: 324.16845703125 | fraction correct: 0.515\n",
      "6020 | loss: 278.4237365722656 | fraction correct: 0.57\n",
      "6030 | loss: 282.26397705078125 | fraction correct: 0.585\n",
      "6040 | loss: 333.0048522949219 | fraction correct: 0.505\n",
      "6050 | loss: 301.86114501953125 | fraction correct: 0.52\n",
      "6060 | loss: 361.17596435546875 | fraction correct: 0.465\n",
      "6070 | loss: 349.0639343261719 | fraction correct: 0.475\n",
      "6080 | loss: 273.3770446777344 | fraction correct: 0.585\n",
      "6090 | loss: 292.3498840332031 | fraction correct: 0.595\n",
      "6100 | loss: 360.4741516113281 | fraction correct: 0.44\n",
      "6110 | loss: 297.3878173828125 | fraction correct: 0.575\n",
      "6120 | loss: 312.64569091796875 | fraction correct: 0.52\n",
      "6130 | loss: 291.0698547363281 | fraction correct: 0.575\n",
      "6140 | loss: 354.6809387207031 | fraction correct: 0.475\n",
      "6150 | loss: 337.4043884277344 | fraction correct: 0.525\n",
      "6160 | loss: 345.91595458984375 | fraction correct: 0.525\n",
      "6170 | loss: 255.74588012695312 | fraction correct: 0.59\n",
      "6180 | loss: 200.03660583496094 | fraction correct: 0.7\n",
      "6190 | loss: 332.3761291503906 | fraction correct: 0.49\n",
      "6200 | loss: 291.7735900878906 | fraction correct: 0.58\n",
      "6210 | loss: 241.9541778564453 | fraction correct: 0.635\n",
      "6220 | loss: 362.13763427734375 | fraction correct: 0.435\n",
      "6230 | loss: 340.85296630859375 | fraction correct: 0.485\n",
      "6240 | loss: 302.98907470703125 | fraction correct: 0.58\n",
      "6250 | loss: 345.7886962890625 | fraction correct: 0.45\n",
      "6260 | loss: 351.4056396484375 | fraction correct: 0.465\n",
      "6270 | loss: 294.1239013671875 | fraction correct: 0.53\n",
      "6280 | loss: 288.8017578125 | fraction correct: 0.555\n",
      "6290 | loss: 318.3751525878906 | fraction correct: 0.505\n",
      "6300 | loss: 286.2173156738281 | fraction correct: 0.56\n",
      "6310 | loss: 344.3338928222656 | fraction correct: 0.46\n",
      "6320 | loss: 231.24246215820312 | fraction correct: 0.605\n",
      "6330 | loss: 373.1097412109375 | fraction correct: 0.46\n",
      "6340 | loss: 276.3956604003906 | fraction correct: 0.62\n",
      "6350 | loss: 306.8318176269531 | fraction correct: 0.54\n",
      "6360 | loss: 310.081298828125 | fraction correct: 0.535\n",
      "6370 | loss: 332.253662109375 | fraction correct: 0.54\n",
      "6380 | loss: 363.17987060546875 | fraction correct: 0.47\n",
      "6390 | loss: 324.97149658203125 | fraction correct: 0.53\n",
      "6400 | loss: 351.4362487792969 | fraction correct: 0.48\n",
      "6410 | loss: 375.9438781738281 | fraction correct: 0.44\n",
      "6420 | loss: 245.2526092529297 | fraction correct: 0.61\n",
      "6430 | loss: 280.567626953125 | fraction correct: 0.58\n",
      "6440 | loss: 335.6017761230469 | fraction correct: 0.495\n",
      "6450 | loss: 403.8965148925781 | fraction correct: 0.405\n",
      "6460 | loss: 321.6626281738281 | fraction correct: 0.515\n",
      "6470 | loss: 325.6597900390625 | fraction correct: 0.52\n",
      "6480 | loss: 347.0050964355469 | fraction correct: 0.49\n",
      "6490 | loss: 266.15655517578125 | fraction correct: 0.635\n",
      "6500 | loss: 325.1447448730469 | fraction correct: 0.535\n",
      "6510 | loss: 310.9172058105469 | fraction correct: 0.5\n",
      "6520 | loss: 385.14971923828125 | fraction correct: 0.41\n",
      "6530 | loss: 258.707763671875 | fraction correct: 0.595\n",
      "6540 | loss: 261.400634765625 | fraction correct: 0.625\n",
      "6550 | loss: 333.4937744140625 | fraction correct: 0.5\n",
      "6560 | loss: 295.13140869140625 | fraction correct: 0.535\n",
      "6570 | loss: 270.2921447753906 | fraction correct: 0.595\n",
      "6580 | loss: 431.0321960449219 | fraction correct: 0.355\n",
      "6590 | loss: 327.0274353027344 | fraction correct: 0.46\n",
      "6600 | loss: 231.55369567871094 | fraction correct: 0.655\n",
      "6610 | loss: 360.3520202636719 | fraction correct: 0.475\n",
      "6620 | loss: 345.68280029296875 | fraction correct: 0.49\n",
      "6630 | loss: 301.8471374511719 | fraction correct: 0.52\n",
      "6640 | loss: 328.1322021484375 | fraction correct: 0.465\n",
      "6650 | loss: 390.18896484375 | fraction correct: 0.41\n",
      "6660 | loss: 334.2210693359375 | fraction correct: 0.55\n",
      "6670 | loss: 342.0455322265625 | fraction correct: 0.48\n",
      "6680 | loss: 331.9336853027344 | fraction correct: 0.465\n",
      "6690 | loss: 325.10650634765625 | fraction correct: 0.48\n",
      "6700 | loss: 301.50933837890625 | fraction correct: 0.5\n",
      "6710 | loss: 337.321044921875 | fraction correct: 0.52\n",
      "6720 | loss: 305.4593505859375 | fraction correct: 0.54\n",
      "6730 | loss: 353.5019836425781 | fraction correct: 0.485\n",
      "6740 | loss: 291.2049865722656 | fraction correct: 0.555\n",
      "6750 | loss: 351.2496337890625 | fraction correct: 0.475\n",
      "6760 | loss: 261.9815979003906 | fraction correct: 0.635\n",
      "6770 | loss: 290.1918640136719 | fraction correct: 0.535\n",
      "6780 | loss: 375.5015563964844 | fraction correct: 0.435\n",
      "6790 | loss: 249.30335998535156 | fraction correct: 0.64\n",
      "6800 | loss: 332.26708984375 | fraction correct: 0.48\n",
      "6810 | loss: 267.3701171875 | fraction correct: 0.6\n",
      "6820 | loss: 296.8145446777344 | fraction correct: 0.55\n",
      "6830 | loss: 297.52825927734375 | fraction correct: 0.525\n",
      "6840 | loss: 299.17840576171875 | fraction correct: 0.545\n",
      "6850 | loss: 311.5792541503906 | fraction correct: 0.545\n",
      "6860 | loss: 220.44068908691406 | fraction correct: 0.695\n",
      "6870 | loss: 321.9494934082031 | fraction correct: 0.49\n",
      "6880 | loss: 295.714599609375 | fraction correct: 0.58\n",
      "6890 | loss: 357.547119140625 | fraction correct: 0.455\n",
      "6900 | loss: 333.7690124511719 | fraction correct: 0.475\n",
      "6910 | loss: 269.7099914550781 | fraction correct: 0.58\n",
      "6920 | loss: 314.38519287109375 | fraction correct: 0.55\n",
      "6930 | loss: 201.87643432617188 | fraction correct: 0.675\n",
      "6940 | loss: 363.8995666503906 | fraction correct: 0.435\n",
      "6950 | loss: 285.093994140625 | fraction correct: 0.55\n",
      "6960 | loss: 315.64404296875 | fraction correct: 0.51\n",
      "6970 | loss: 305.7590026855469 | fraction correct: 0.515\n",
      "6980 | loss: 285.06158447265625 | fraction correct: 0.555\n",
      "6990 | loss: 303.31597900390625 | fraction correct: 0.54\n",
      "7000 | loss: 212.81997680664062 | fraction correct: 0.69\n",
      "7010 | loss: 346.7251892089844 | fraction correct: 0.5\n",
      "7020 | loss: 299.3884582519531 | fraction correct: 0.53\n",
      "7030 | loss: 309.2289733886719 | fraction correct: 0.56\n",
      "7040 | loss: 277.0335693359375 | fraction correct: 0.605\n",
      "7050 | loss: 313.62091064453125 | fraction correct: 0.53\n",
      "7060 | loss: 337.9344177246094 | fraction correct: 0.47\n",
      "7070 | loss: 315.0039367675781 | fraction correct: 0.53\n",
      "7080 | loss: 323.5752868652344 | fraction correct: 0.555\n",
      "7090 | loss: 292.9218444824219 | fraction correct: 0.535\n",
      "7100 | loss: 248.9132537841797 | fraction correct: 0.65\n",
      "7110 | loss: 264.3185729980469 | fraction correct: 0.585\n",
      "7120 | loss: 350.0167236328125 | fraction correct: 0.505\n",
      "7130 | loss: 305.0432434082031 | fraction correct: 0.535\n",
      "7140 | loss: 305.72235107421875 | fraction correct: 0.51\n",
      "7150 | loss: 307.9747619628906 | fraction correct: 0.54\n",
      "7160 | loss: 335.93402099609375 | fraction correct: 0.46\n",
      "7170 | loss: 227.9501953125 | fraction correct: 0.645\n",
      "7180 | loss: 386.3668212890625 | fraction correct: 0.42\n",
      "7190 | loss: 305.7179260253906 | fraction correct: 0.55\n",
      "7200 | loss: 350.73040771484375 | fraction correct: 0.45\n",
      "7210 | loss: 240.83729553222656 | fraction correct: 0.61\n",
      "7220 | loss: 287.8751220703125 | fraction correct: 0.575\n",
      "7230 | loss: 332.01141357421875 | fraction correct: 0.485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7240 | loss: 357.0703430175781 | fraction correct: 0.425\n",
      "7250 | loss: 256.7506408691406 | fraction correct: 0.61\n",
      "7260 | loss: 321.7571716308594 | fraction correct: 0.5\n",
      "7270 | loss: 373.284423828125 | fraction correct: 0.43\n",
      "7280 | loss: 240.58944702148438 | fraction correct: 0.655\n",
      "7290 | loss: 293.5312805175781 | fraction correct: 0.565\n",
      "7300 | loss: 317.0809326171875 | fraction correct: 0.52\n",
      "7310 | loss: 253.7042236328125 | fraction correct: 0.63\n",
      "7320 | loss: 233.6385498046875 | fraction correct: 0.64\n",
      "7330 | loss: 302.95843505859375 | fraction correct: 0.54\n",
      "7340 | loss: 335.05517578125 | fraction correct: 0.48\n",
      "7350 | loss: 299.828125 | fraction correct: 0.555\n",
      "7360 | loss: 206.273193359375 | fraction correct: 0.69\n",
      "7370 | loss: 373.3910217285156 | fraction correct: 0.435\n",
      "7380 | loss: 383.96685791015625 | fraction correct: 0.425\n",
      "7390 | loss: 333.71551513671875 | fraction correct: 0.51\n",
      "7400 | loss: 363.4162292480469 | fraction correct: 0.4\n",
      "7410 | loss: 352.8750305175781 | fraction correct: 0.46\n",
      "7420 | loss: 300.33575439453125 | fraction correct: 0.57\n",
      "7430 | loss: 229.1928253173828 | fraction correct: 0.63\n",
      "7440 | loss: 294.8492736816406 | fraction correct: 0.58\n",
      "7450 | loss: 324.69287109375 | fraction correct: 0.535\n",
      "7460 | loss: 267.7760314941406 | fraction correct: 0.615\n",
      "7470 | loss: 324.3877868652344 | fraction correct: 0.56\n",
      "7480 | loss: 351.7645568847656 | fraction correct: 0.485\n",
      "7490 | loss: 305.49481201171875 | fraction correct: 0.55\n",
      "7500 | loss: 251.92425537109375 | fraction correct: 0.595\n",
      "7510 | loss: 341.22625732421875 | fraction correct: 0.475\n",
      "7520 | loss: 288.8506164550781 | fraction correct: 0.56\n",
      "7530 | loss: 315.27862548828125 | fraction correct: 0.51\n",
      "7540 | loss: 257.7734680175781 | fraction correct: 0.625\n",
      "7550 | loss: 269.10894775390625 | fraction correct: 0.57\n",
      "7560 | loss: 331.8517761230469 | fraction correct: 0.55\n",
      "7570 | loss: 261.0735168457031 | fraction correct: 0.56\n",
      "7580 | loss: 308.5495910644531 | fraction correct: 0.485\n",
      "7590 | loss: 263.55523681640625 | fraction correct: 0.62\n",
      "7600 | loss: 298.8288879394531 | fraction correct: 0.545\n",
      "7610 | loss: 281.9886779785156 | fraction correct: 0.57\n",
      "7620 | loss: 254.3731689453125 | fraction correct: 0.635\n",
      "7630 | loss: 351.6766052246094 | fraction correct: 0.46\n",
      "7640 | loss: 322.8984680175781 | fraction correct: 0.48\n",
      "7650 | loss: 336.0679016113281 | fraction correct: 0.51\n",
      "7660 | loss: 350.9723815917969 | fraction correct: 0.52\n",
      "7670 | loss: 287.5530090332031 | fraction correct: 0.485\n",
      "7680 | loss: 296.1450500488281 | fraction correct: 0.555\n",
      "7690 | loss: 319.10968017578125 | fraction correct: 0.51\n",
      "7700 | loss: 337.9066162109375 | fraction correct: 0.46\n",
      "7710 | loss: 275.1665954589844 | fraction correct: 0.555\n",
      "7720 | loss: 301.64154052734375 | fraction correct: 0.535\n",
      "7730 | loss: 246.32615661621094 | fraction correct: 0.64\n",
      "7740 | loss: 285.6404113769531 | fraction correct: 0.54\n",
      "7750 | loss: 305.814453125 | fraction correct: 0.51\n",
      "7760 | loss: 293.9361877441406 | fraction correct: 0.53\n",
      "7770 | loss: 342.1861572265625 | fraction correct: 0.475\n",
      "7780 | loss: 236.67892456054688 | fraction correct: 0.66\n",
      "7790 | loss: 298.5044250488281 | fraction correct: 0.545\n",
      "7800 | loss: 378.86590576171875 | fraction correct: 0.445\n",
      "7810 | loss: 251.49249267578125 | fraction correct: 0.62\n",
      "7820 | loss: 285.6256103515625 | fraction correct: 0.53\n",
      "7830 | loss: 343.4479675292969 | fraction correct: 0.465\n",
      "7840 | loss: 363.41375732421875 | fraction correct: 0.46\n",
      "7850 | loss: 284.6134338378906 | fraction correct: 0.6\n",
      "7860 | loss: 271.70367431640625 | fraction correct: 0.63\n",
      "7870 | loss: 273.0494384765625 | fraction correct: 0.555\n",
      "7880 | loss: 337.6161193847656 | fraction correct: 0.505\n",
      "7890 | loss: 267.748779296875 | fraction correct: 0.615\n",
      "7900 | loss: 326.22918701171875 | fraction correct: 0.46\n",
      "7910 | loss: 299.74517822265625 | fraction correct: 0.545\n",
      "7920 | loss: 300.1463623046875 | fraction correct: 0.48\n",
      "7930 | loss: 221.1668243408203 | fraction correct: 0.695\n",
      "7940 | loss: 356.900634765625 | fraction correct: 0.44\n",
      "7950 | loss: 307.6871032714844 | fraction correct: 0.51\n",
      "7960 | loss: 300.27349853515625 | fraction correct: 0.565\n",
      "7970 | loss: 326.9375 | fraction correct: 0.52\n",
      "7980 | loss: 320.95806884765625 | fraction correct: 0.505\n",
      "7990 | loss: 336.47265625 | fraction correct: 0.48\n",
      "8000 | loss: 344.112060546875 | fraction correct: 0.51\n",
      "8010 | loss: 279.85052490234375 | fraction correct: 0.62\n",
      "8020 | loss: 311.1884765625 | fraction correct: 0.515\n",
      "8030 | loss: 319.083740234375 | fraction correct: 0.525\n",
      "8040 | loss: 275.778076171875 | fraction correct: 0.635\n",
      "8050 | loss: 274.5479736328125 | fraction correct: 0.565\n",
      "8060 | loss: 301.59033203125 | fraction correct: 0.565\n",
      "8070 | loss: 219.42791748046875 | fraction correct: 0.67\n",
      "8080 | loss: 282.9037170410156 | fraction correct: 0.555\n",
      "8090 | loss: 331.86297607421875 | fraction correct: 0.525\n",
      "8100 | loss: 353.46673583984375 | fraction correct: 0.48\n",
      "8110 | loss: 365.6286926269531 | fraction correct: 0.45\n",
      "8120 | loss: 347.1844177246094 | fraction correct: 0.46\n",
      "8130 | loss: 288.7101745605469 | fraction correct: 0.53\n",
      "8140 | loss: 341.6961364746094 | fraction correct: 0.46\n",
      "8150 | loss: 266.3966979980469 | fraction correct: 0.61\n",
      "8160 | loss: 362.95166015625 | fraction correct: 0.42\n",
      "8170 | loss: 243.14447021484375 | fraction correct: 0.67\n",
      "8180 | loss: 337.0554504394531 | fraction correct: 0.505\n",
      "8190 | loss: 370.634521484375 | fraction correct: 0.455\n",
      "8200 | loss: 346.8362731933594 | fraction correct: 0.455\n",
      "8210 | loss: 307.8140563964844 | fraction correct: 0.58\n",
      "8220 | loss: 360.5783386230469 | fraction correct: 0.425\n",
      "8230 | loss: 372.3308410644531 | fraction correct: 0.435\n",
      "8240 | loss: 239.605712890625 | fraction correct: 0.625\n",
      "8250 | loss: 330.36846923828125 | fraction correct: 0.57\n",
      "8260 | loss: 337.41839599609375 | fraction correct: 0.5\n",
      "8270 | loss: 278.0457763671875 | fraction correct: 0.595\n",
      "8280 | loss: 309.6280212402344 | fraction correct: 0.54\n",
      "8290 | loss: 266.8468322753906 | fraction correct: 0.555\n",
      "8300 | loss: 254.56300354003906 | fraction correct: 0.615\n",
      "8310 | loss: 275.26129150390625 | fraction correct: 0.595\n",
      "8320 | loss: 345.587646484375 | fraction correct: 0.515\n",
      "8330 | loss: 293.6502990722656 | fraction correct: 0.565\n",
      "8340 | loss: 308.50286865234375 | fraction correct: 0.57\n",
      "8350 | loss: 323.60906982421875 | fraction correct: 0.485\n",
      "8360 | loss: 264.6817932128906 | fraction correct: 0.595\n",
      "8370 | loss: 361.9775390625 | fraction correct: 0.475\n",
      "8380 | loss: 361.22808837890625 | fraction correct: 0.475\n",
      "8390 | loss: 374.77862548828125 | fraction correct: 0.43\n",
      "8400 | loss: 252.21884155273438 | fraction correct: 0.63\n",
      "8410 | loss: 259.3587951660156 | fraction correct: 0.595\n",
      "8420 | loss: 271.16943359375 | fraction correct: 0.64\n",
      "8430 | loss: 226.85902404785156 | fraction correct: 0.66\n",
      "8440 | loss: 323.7164306640625 | fraction correct: 0.495\n",
      "8450 | loss: 292.1470642089844 | fraction correct: 0.59\n",
      "8460 | loss: 320.0129699707031 | fraction correct: 0.52\n",
      "8470 | loss: 296.3843688964844 | fraction correct: 0.565\n",
      "8480 | loss: 343.9364929199219 | fraction correct: 0.45\n",
      "8490 | loss: 344.1714782714844 | fraction correct: 0.435\n",
      "8500 | loss: 273.2772216796875 | fraction correct: 0.535\n",
      "8510 | loss: 257.265869140625 | fraction correct: 0.625\n",
      "8520 | loss: 322.9744873046875 | fraction correct: 0.54\n",
      "8530 | loss: 353.9956970214844 | fraction correct: 0.475\n",
      "8540 | loss: 342.7628173828125 | fraction correct: 0.505\n",
      "8550 | loss: 328.9858703613281 | fraction correct: 0.47\n",
      "8560 | loss: 301.0483093261719 | fraction correct: 0.54\n",
      "8570 | loss: 376.5384521484375 | fraction correct: 0.42\n",
      "8580 | loss: 349.50250244140625 | fraction correct: 0.475\n",
      "8590 | loss: 279.9942321777344 | fraction correct: 0.635\n",
      "8600 | loss: 372.0626220703125 | fraction correct: 0.47\n",
      "8610 | loss: 288.1822204589844 | fraction correct: 0.53\n",
      "8620 | loss: 270.07489013671875 | fraction correct: 0.625\n",
      "8630 | loss: 317.2689514160156 | fraction correct: 0.52\n",
      "8640 | loss: 312.3868713378906 | fraction correct: 0.51\n",
      "8650 | loss: 307.35772705078125 | fraction correct: 0.545\n",
      "8660 | loss: 275.3827209472656 | fraction correct: 0.555\n",
      "8670 | loss: 371.2154235839844 | fraction correct: 0.435\n",
      "8680 | loss: 330.0113220214844 | fraction correct: 0.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8690 | loss: 313.5472106933594 | fraction correct: 0.54\n",
      "8700 | loss: 291.39532470703125 | fraction correct: 0.55\n",
      "8710 | loss: 257.85186767578125 | fraction correct: 0.61\n",
      "8720 | loss: 297.96502685546875 | fraction correct: 0.565\n",
      "8730 | loss: 303.41583251953125 | fraction correct: 0.52\n",
      "8740 | loss: 369.5868835449219 | fraction correct: 0.455\n",
      "8750 | loss: 287.97735595703125 | fraction correct: 0.575\n",
      "8760 | loss: 305.09417724609375 | fraction correct: 0.54\n",
      "8770 | loss: 367.17352294921875 | fraction correct: 0.485\n",
      "8780 | loss: 336.27520751953125 | fraction correct: 0.465\n",
      "8790 | loss: 288.6126403808594 | fraction correct: 0.565\n",
      "8800 | loss: 247.03121948242188 | fraction correct: 0.63\n",
      "8810 | loss: 262.2066345214844 | fraction correct: 0.58\n",
      "8820 | loss: 353.78668212890625 | fraction correct: 0.455\n",
      "8830 | loss: 249.69403076171875 | fraction correct: 0.585\n",
      "8840 | loss: 333.04486083984375 | fraction correct: 0.525\n",
      "8850 | loss: 389.7028503417969 | fraction correct: 0.45\n",
      "8860 | loss: 308.6750183105469 | fraction correct: 0.55\n",
      "8870 | loss: 275.7611083984375 | fraction correct: 0.595\n",
      "8880 | loss: 255.30325317382812 | fraction correct: 0.61\n",
      "8890 | loss: 253.9345245361328 | fraction correct: 0.635\n",
      "8900 | loss: 308.36260986328125 | fraction correct: 0.56\n",
      "8910 | loss: 321.8341369628906 | fraction correct: 0.57\n",
      "8920 | loss: 288.733642578125 | fraction correct: 0.52\n",
      "8930 | loss: 285.916015625 | fraction correct: 0.54\n",
      "8940 | loss: 293.388916015625 | fraction correct: 0.545\n",
      "8950 | loss: 243.4676971435547 | fraction correct: 0.67\n",
      "8960 | loss: 329.72650146484375 | fraction correct: 0.465\n",
      "8970 | loss: 309.4219665527344 | fraction correct: 0.51\n",
      "8980 | loss: 339.3883361816406 | fraction correct: 0.51\n",
      "8990 | loss: 366.1171875 | fraction correct: 0.475\n",
      "9000 | loss: 354.379150390625 | fraction correct: 0.455\n",
      "9010 | loss: 267.6963195800781 | fraction correct: 0.575\n",
      "9020 | loss: 361.17596435546875 | fraction correct: 0.445\n",
      "9030 | loss: 388.65948486328125 | fraction correct: 0.41\n",
      "9040 | loss: 249.82684326171875 | fraction correct: 0.59\n",
      "9050 | loss: 387.1997375488281 | fraction correct: 0.45\n",
      "9060 | loss: 304.4980773925781 | fraction correct: 0.54\n",
      "9070 | loss: 349.4539794921875 | fraction correct: 0.47\n",
      "9080 | loss: 331.1380310058594 | fraction correct: 0.52\n",
      "9090 | loss: 305.773193359375 | fraction correct: 0.515\n",
      "9100 | loss: 279.19549560546875 | fraction correct: 0.595\n",
      "9110 | loss: 255.37991333007812 | fraction correct: 0.57\n",
      "9120 | loss: 192.8680877685547 | fraction correct: 0.69\n",
      "9130 | loss: 332.2101135253906 | fraction correct: 0.515\n",
      "9140 | loss: 341.7817077636719 | fraction correct: 0.49\n",
      "9150 | loss: 300.7453308105469 | fraction correct: 0.495\n",
      "9160 | loss: 231.60650634765625 | fraction correct: 0.67\n",
      "9170 | loss: 385.1182556152344 | fraction correct: 0.45\n",
      "9180 | loss: 314.18121337890625 | fraction correct: 0.53\n",
      "9190 | loss: 227.90469360351562 | fraction correct: 0.67\n",
      "9200 | loss: 286.746337890625 | fraction correct: 0.555\n",
      "9210 | loss: 313.1942443847656 | fraction correct: 0.54\n",
      "9220 | loss: 247.95387268066406 | fraction correct: 0.61\n",
      "9230 | loss: 319.07928466796875 | fraction correct: 0.56\n",
      "9240 | loss: 321.8741760253906 | fraction correct: 0.49\n",
      "9250 | loss: 324.9151611328125 | fraction correct: 0.475\n",
      "9260 | loss: 289.0415954589844 | fraction correct: 0.57\n",
      "9270 | loss: 320.9733581542969 | fraction correct: 0.51\n",
      "9280 | loss: 292.60809326171875 | fraction correct: 0.58\n",
      "9290 | loss: 295.8905029296875 | fraction correct: 0.56\n",
      "9300 | loss: 197.4733428955078 | fraction correct: 0.72\n",
      "9310 | loss: 355.6088562011719 | fraction correct: 0.47\n",
      "9320 | loss: 366.49884033203125 | fraction correct: 0.45\n",
      "9330 | loss: 285.1713562011719 | fraction correct: 0.57\n",
      "9340 | loss: 235.3303985595703 | fraction correct: 0.65\n",
      "9350 | loss: 248.61737060546875 | fraction correct: 0.63\n",
      "9360 | loss: 339.1812438964844 | fraction correct: 0.515\n",
      "9370 | loss: 315.4736022949219 | fraction correct: 0.475\n",
      "9380 | loss: 371.9056091308594 | fraction correct: 0.455\n",
      "9390 | loss: 285.8043212890625 | fraction correct: 0.55\n",
      "9400 | loss: 286.2027282714844 | fraction correct: 0.535\n",
      "9410 | loss: 294.74884033203125 | fraction correct: 0.55\n",
      "9420 | loss: 289.0591125488281 | fraction correct: 0.565\n",
      "9430 | loss: 301.51434326171875 | fraction correct: 0.535\n",
      "9440 | loss: 347.9277038574219 | fraction correct: 0.46\n",
      "9450 | loss: 274.2157287597656 | fraction correct: 0.605\n",
      "9460 | loss: 317.5271301269531 | fraction correct: 0.555\n",
      "9470 | loss: 348.7134094238281 | fraction correct: 0.46\n",
      "9480 | loss: 216.61260986328125 | fraction correct: 0.655\n",
      "9490 | loss: 316.82879638671875 | fraction correct: 0.475\n",
      "9500 | loss: 314.53802490234375 | fraction correct: 0.545\n",
      "9510 | loss: 270.5320129394531 | fraction correct: 0.585\n",
      "9520 | loss: 288.968994140625 | fraction correct: 0.555\n",
      "9530 | loss: 298.3685302734375 | fraction correct: 0.57\n",
      "9540 | loss: 351.0490417480469 | fraction correct: 0.49\n",
      "9550 | loss: 287.1003112792969 | fraction correct: 0.615\n",
      "9560 | loss: 327.3966064453125 | fraction correct: 0.525\n",
      "9570 | loss: 249.55714416503906 | fraction correct: 0.585\n",
      "9580 | loss: 313.2640686035156 | fraction correct: 0.5\n",
      "9590 | loss: 352.2100524902344 | fraction correct: 0.465\n",
      "9600 | loss: 368.3971252441406 | fraction correct: 0.43\n",
      "9610 | loss: 297.1039733886719 | fraction correct: 0.545\n",
      "9620 | loss: 362.470458984375 | fraction correct: 0.425\n",
      "9630 | loss: 304.053466796875 | fraction correct: 0.505\n",
      "9640 | loss: 293.3541259765625 | fraction correct: 0.565\n",
      "9650 | loss: 349.4499206542969 | fraction correct: 0.48\n",
      "9660 | loss: 310.2057800292969 | fraction correct: 0.525\n",
      "9670 | loss: 257.7200622558594 | fraction correct: 0.59\n",
      "9680 | loss: 269.1197204589844 | fraction correct: 0.625\n",
      "9690 | loss: 284.2832946777344 | fraction correct: 0.58\n",
      "9700 | loss: 307.1241149902344 | fraction correct: 0.545\n",
      "9710 | loss: 282.6732177734375 | fraction correct: 0.58\n",
      "9720 | loss: 307.3559875488281 | fraction correct: 0.565\n",
      "9730 | loss: 323.6058654785156 | fraction correct: 0.51\n",
      "9740 | loss: 306.66705322265625 | fraction correct: 0.55\n",
      "9750 | loss: 272.3097229003906 | fraction correct: 0.555\n",
      "9760 | loss: 240.92156982421875 | fraction correct: 0.655\n",
      "9770 | loss: 271.9346923828125 | fraction correct: 0.59\n",
      "9780 | loss: 311.813232421875 | fraction correct: 0.52\n",
      "9790 | loss: 234.71815490722656 | fraction correct: 0.65\n",
      "9800 | loss: 324.87713623046875 | fraction correct: 0.485\n",
      "9810 | loss: 351.1169738769531 | fraction correct: 0.495\n",
      "9820 | loss: 338.8500061035156 | fraction correct: 0.47\n",
      "9830 | loss: 363.89959716796875 | fraction correct: 0.445\n",
      "9840 | loss: 248.0940704345703 | fraction correct: 0.615\n",
      "9850 | loss: 340.0087585449219 | fraction correct: 0.48\n",
      "9860 | loss: 344.25244140625 | fraction correct: 0.475\n",
      "9870 | loss: 348.37371826171875 | fraction correct: 0.475\n",
      "9880 | loss: 355.6662292480469 | fraction correct: 0.495\n",
      "9890 | loss: 249.65232849121094 | fraction correct: 0.615\n",
      "9900 | loss: 313.8795471191406 | fraction correct: 0.485\n",
      "9910 | loss: 279.6518249511719 | fraction correct: 0.565\n",
      "9920 | loss: 402.728759765625 | fraction correct: 0.385\n",
      "9930 | loss: 304.6111145019531 | fraction correct: 0.605\n",
      "9940 | loss: 318.4808349609375 | fraction correct: 0.495\n",
      "9950 | loss: 336.5570983886719 | fraction correct: 0.49\n",
      "9960 | loss: 323.93096923828125 | fraction correct: 0.54\n",
      "9970 | loss: 337.6738586425781 | fraction correct: 0.485\n",
      "9980 | loss: 230.0484619140625 | fraction correct: 0.645\n",
      "9990 | loss: 246.88316345214844 | fraction correct: 0.625\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import mnist_loader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Download the data (thanks MIT)\n",
    "response = requests.get(\"http://shakespeare.mit.edu/hamlet/full.html\")\n",
    "hamlet = BeautifulSoup(response.content, \"html.parser\").getText()\n",
    "\n",
    "# Convert text to character-level one-hot encoding\n",
    "hamlet_one_hot = pd.get_dummies(pd.Series(list(hamlet)))\n",
    "character_vec = hamlet_one_hot.columns\n",
    "x = torch.from_numpy(hamlet_one_hot.values.astype(np.float32))\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Neural network layers. Rather than giving the input as a one-hot\n",
    "        # vector, we represent it as a point in a high-dimensional space (i.e.\n",
    "        # \"an embedding\"). This tends to work better.\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        signal = self.encoder(input_).view(1, 1, -1)  # Embed input character\n",
    "        output, hidden = self.rnn(signal, hidden)     # Get output and hidden vector(s)\n",
    "        prediction = self.decoder(output.view(1, -1))     # Decode to \"prediction\" vector\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            torch.autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)),\n",
    "            torch.autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        )\n",
    "    \n",
    "epochs = 10000\n",
    "seq_len = 200\n",
    "learning_rate = 1e-2\n",
    "n_layers = 2\n",
    "\n",
    "model = RNN(len(character_vec), 100, len(character_vec), n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# We're collecting the losses so we can plot how they (hopefully) decrease over time\n",
    "all_losses = []\n",
    "fraction_correct = []\n",
    "for t in tqdm(range(epochs)):\n",
    "    \n",
    "    # Initiate a hidden vector for \n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # Pick a random input and output sequence. Here we are only taking one sequence\n",
    "    # per epoch, but normally people take a batch of sequences. Check https://github\n",
    "    # .com/spro/char-rnn.pytorch for an example on how to do that\n",
    "    i = np.random.randint(0, x.size(0)-seq_len)\n",
    "    input_ = torch.max(x[i:i+seq_len], 1)[1]      # torch.max(...)[1] gets the character in terms of its index, so\n",
    "    target = torch.max(x[i+1:i+1+seq_len], 1)[1]  # input_ will be something like torch.tensor([2, 1, 4, 0, ... 1])\n",
    "    \n",
    "    # Backprop through time. Here we are just summing the losses from each timestep\n",
    "    # and do backpropagation on the variable that holds the sum. PyTorch allows for that\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for j in range(input_.size(0)):\n",
    "        output, hidden = model(input_[j], hidden)\n",
    "        loss += loss_fn(output.view(1, -1), target[j].view(1, ))\n",
    "        correct += int(torch.max(output.view(1, -1), 1)[1][0] == target[j])\n",
    "    \n",
    "    # SGD step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Collect loss value for plot\n",
    "    all_losses.append(float(loss))\n",
    "    fraction_correct.append(correct / input_.size(0))\n",
    "        \n",
    "    # Progress\n",
    "    if t % 10 == 0:\n",
    "        print(t, \"| loss:\", float(loss), \"| fraction correct:\", fraction_correct[-1])\n",
    "    \n",
    "    \n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.2.1**: Train the network for a while (the longer the better) until you feel its error has settled in some local minimum. Then go ahead and generate some gibberish Hamlet with it! To get better results, you can \"warm up\" the hidden state vectors by first running a sequence of actual Shapespeare through it and then starting generating from the last word in that sequence. Also, what I mean by \"start generating\" is that instead of predicting output from inputs drawn from your dataset, you input the prediction from the previous timestep and repeat, thus getting something that's completely made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2677d3c6b00>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sina's training: Plot losses with 4000 epochs\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "\n",
      "What he shall the strace so the straction the straction,\n",
      "And the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall \n",
      "the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he sh\n",
      "all the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As h\n",
      "e shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the stract\n",
      "ion\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the st\n",
      "raction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction th\n",
      "e straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stractio\n",
      "n the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stra\n",
      "ction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the \n",
      "straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction \n",
      "the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the stract\n",
      "ion the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the st\n",
      "raction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so th\n",
      "e straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace s\n",
      "o the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the stra\n",
      "ce so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the \n",
      "strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall \n",
      "the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he sh\n",
      "all the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As h\n",
      "e shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the stract\n",
      "ion\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the st\n",
      "raction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction th\n",
      "e straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stractio\n",
      "n the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stra\n",
      "ction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the \n",
      "straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction \n",
      "the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the stract\n",
      "ion the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the st\n",
      "raction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so th\n",
      "e straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace s\n",
      "o the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the stra\n",
      "ce so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the \n",
      "strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall \n",
      "the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he sh\n",
      "all the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As h\n",
      "e shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the stract\n",
      "ion\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the st\n",
      "raction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction th\n",
      "e straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stractio\n",
      "n the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the stra\n",
      "ction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the \n",
      "straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction \n",
      "the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the stract\n",
      "ion the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the st\n",
      "raction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so th\n",
      "e straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace s\n",
      "o the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the strace so the straction the straction the straction\n",
      "As he shall the stra\n"
     ]
    }
   ],
   "source": [
    "#hidden = model.init_hidden()\n",
    "\n",
    "for n in range(100):\n",
    "    i = np.random.randint(0, x.size(0)-seq_len)\n",
    "    input_ = torch.max(x[i:i+seq_len], 1)[1]      # torch.max(...)[1] gets the character in terms of its index, so\n",
    "\n",
    "    for j in range(input_.size(0)):\n",
    "        output, hidden = model(input_[j], hidden)\n",
    "        \n",
    "        \n",
    "out_int=torch.max(output.view(1, -1), 1)[1][0]\n",
    "\n",
    "for i in range(50):\n",
    "    output_full=''\n",
    "    for j in range(input_.size(0)):\n",
    "        output, hidden = model(out_int, hidden)\n",
    "        out_int = torch.max(output.view(1, -1), 1)[1][0]\n",
    "        \n",
    "        output_full=output_full+character_vec[int(out_int)]\n",
    "    print(output_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
